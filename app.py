import re
import io
import requests
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt
from datetime import date, datetime
import pdfplumber
import tempfile
from openpyxl import Workbook
from openpyxl.drawing.image import Image as XLImage
from openpyxl.utils.dataframe import dataframe_to_rows


# ----------------------------
# Page / UI
# ----------------------------
st.set_page_config(
    page_title="åŸä¾¡ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ï¼ˆéŠ…ãƒ»ã‚¢ãƒ«ãƒŸãƒ»é‹è³ƒãƒ»è³ƒé‡‘ï¼‰",
    page_icon="ğŸ“ˆ",
    layout="wide",
)

st.title("ğŸ“ˆ åŸä¾¡ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ï¼ˆéŠ…ãƒ»ã‚¢ãƒ«ãƒŸãƒ»é‹è³ƒãƒ»è³ƒé‡‘ï¼‰")
st.caption("éŠ…/ã‚¢ãƒ«ãƒŸã¯FREDï¼ˆUSD/tonï¼‰Ã—USDJPYã§å††/kgæ›ç®—ã€‚é‹è³ƒã¯WebKIT PDFã€‚è³ƒé‡‘ã¯e-Stat APIã€‚")

with st.sidebar:
    st.header("ğŸ“š ãƒ‡ãƒ¼ã‚¿å…ƒãƒªãƒ³ã‚¯ï¼ˆå›ºå®šè¡¨ç¤ºï¼‰")
    st.markdown("""
**â–  FRED**
- éŠ…ï¼ˆUSD/tonï¼‰: https://fred.stlouisfed.org/series/PCOPPUSDM  
- ã‚¢ãƒ«ãƒŸï¼ˆUSD/tonï¼‰: https://fred.stlouisfed.org/series/PALUMUSDM  
- ç‚ºæ›¿ï¼ˆUSD/JPYï¼‰: https://fred.stlouisfed.org/series/EXJPUS  

**â–  WebKITï¼ˆå…¨ãƒˆå”ï¼‰**
- å…¬è¡¨ãƒšãƒ¼ã‚¸: https://jta.or.jp/member/keiei/kit_release.html  

**â–  e-Stat API**
- ä»•æ§˜/æ¡ˆå†…: https://www.e-stat.go.jp/api/api-info/e-stat-manual  
""")

with st.expander("ã“ã®ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã«ã¤ã„ã¦", expanded=True):
    st.markdown("""
- **éŠ…ãƒ»ã‚¢ãƒ«ãƒŸï¼ˆå††/kgï¼‰**ï¼šFREDã®æœˆæ¬¡ï¼ˆUSD/tonï¼‰Ã— USDJPY Ã· 1000  
- **é‹è³ƒæŒ‡æ•°**ï¼šWebKITæˆç´„é‹è³ƒæŒ‡æ•°ï¼ˆPDFå†…ã®æœˆåˆ¥è¡¨ï¼‰  
- **è³ƒé‡‘ï¼ˆè£½é€ æ¥­ï¼‰**ï¼še-Stat APIã‹ã‚‰å–å¾—ï¼ˆçµ±è¨ˆè¡¨ID=statsDataId ã‚’æ¤œç´¢ã—ã¦åˆ©ç”¨ï¼‰  
""")

# ----------------------------
# Secrets
# ----------------------------
FRED_API_KEY = st.secrets.get("FRED_API_KEY", "")
if not FRED_API_KEY:
    st.error("FRED_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼ˆStreamlit Secrets ã‚’ç¢ºèªï¼‰")
    st.stop()

ESTAT_APP_ID = st.secrets.get("ESTAT_APP_ID", "")
if not ESTAT_APP_ID:
    st.warning("ESTAT_APP_ID ãŒæœªè¨­å®šã§ã™ï¼ˆè³ƒé‡‘ãƒ‘ãƒ¼ãƒˆã¯å‹•ãã¾ã›ã‚“ï¼‰ã€‚Streamlit Secrets ã«è¿½åŠ ã—ã¦ãã ã•ã„ã€‚")

# ----------------------------
# FRED fetch
# ----------------------------
@st.cache_data(ttl=60 * 60)
def fetch_fred(series_id: str, start: str = "2018-01-01") -> pd.Series:
    url = "https://api.stlouisfed.org/fred/series/observations"
    params = {
        "series_id": series_id,
        "api_key": FRED_API_KEY,
        "file_type": "json",
        "observation_start": start,
        "observation_end": date.today().isoformat(),
    }
    r = requests.get(url, params=params, timeout=30)
    r.raise_for_status()
    obs = r.json()["observations"]

    df = pd.DataFrame(obs)[["date", "value"]]
    df["date"] = pd.to_datetime(df["date"])
    df["value"] = pd.to_numeric(df["value"], errors="coerce")
    s = df.dropna().set_index("date")["value"].sort_index()
    return s

# ----------------------------
# WebKIT (latest pdf URL from JTA page)
# ----------------------------
@st.cache_data(ttl=60 * 60)
def webkit_latest_pdf_url() -> str:
    # JTAã®ãƒšãƒ¼ã‚¸ã«æ¯æœˆã®PDFãƒªãƒ³ã‚¯ãŒè¼‰ã‚‹ï¼ˆä¾‹: /pdf/kit_release/202512.pdfï¼‰
    url = "https://jta.or.jp/member/keiei/kit_release.html"
    r = requests.get(url, timeout=30)
    r.raise_for_status()
    html = r.text

    links = re.findall(r"/pdf/kit_release/(\d{6})\.pdf", html)
    if not links:
        raise ValueError("WebKITã®PDFãƒªãƒ³ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ãƒšãƒ¼ã‚¸æ§‹é€ ãŒå¤‰ã‚ã£ãŸå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")

    latest_yyyymm = max(links)  # æ–‡å­—åˆ—æ¯”è¼ƒã§OKï¼ˆYYYYMMï¼‰
    return f"https://jta.or.jp/pdf/kit_release/{latest_yyyymm}.pdf"

@st.cache_data(ttl=60 * 60)
def fetch_webkit_index_from_pdf(pdf_url: str) -> pd.Series:
    r = requests.get(pdf_url, timeout=60)
    r.raise_for_status()

    with pdfplumber.open(io.BytesIO(r.content)) as pdf:
        text = "\n".join((page.extract_text() or "") for page in pdf.pages)

    # ã€Œæˆç´„é‹è³ƒæŒ‡æ•°ï¼ˆæœˆåˆ¥ï¼‰ã®æ¨ç§»ã€ã®è¡¨ã¯
    # å¹³æˆï¼’ï¼’å¹´åº¦ 100 98 ... ã®ã‚ˆã†ãªè¡Œã§å‡ºã¦ãã‚‹ï¼ˆPDFã®ãƒšãƒ¼ã‚¸2ä»˜è¿‘ï¼‰ :contentReference[oaicite:2]{index=2}
    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]

    # ã€Œå¹³æˆã€ã€Œä»¤å’Œã€ã©ã¡ã‚‰ã‚‚æ‹¾ã†ï¼ˆå…¨è§’æ•°å­—ã§ã‚‚OKã«ã™ã‚‹ãŸã‚ \d ã§ã¯ãªãæ•°å­—æŠ½å‡ºæ–¹å¼ï¼‰
    target_rows = []
    for ln in lines:
        if re.match(r"^(å¹³æˆ|ä»¤å’Œ).+å¹´åº¦", ln):
            target_rows.append(ln)

    if not target_rows:
        return pd.Series(dtype="float64")

    data_points = []
    for ln in target_rows:
        # è¡Œã‹ã‚‰æ•°å€¤ã‚’å…¨éƒ¨æ‹¾ã†ï¼ˆå¹´åº¦åã®ä¸­ã®æ•°å­—ã‚‚æ‹¾ã†ã®ã§ã€12å€‹ã ã‘ä½¿ã†ï¼‰
        nums = re.findall(r"\d+", ln)
        if len(nums) < 12:
            continue


        # å¹´åº¦ãƒ©ãƒ™ãƒ«æŠ½å‡º
        m = re.search(r"(å¹³æˆ|ä»¤å’Œ)\s*([0-9]+)\s*å¹´åº¦", ln)
        if not m:
            m = re.search(r"(å¹³æˆ|ä»¤å’Œ)([0-9]+)å¹´åº¦", ln)
        if not m:
            continue

        era = m.group(1)
        n = int(m.group(2))

        # å¹´åº¦é–‹å§‹ã®è¥¿æš¦ï¼ˆ4æœˆé–‹å§‹ï¼‰
        if era == "ä»¤å’Œ":
            start_year = 2018 + n   # ä»¤å’Œ1=2019
        else:
            start_year = 1988 + n   # å¹³æˆ1=1989

        # ã“ã®è¡Œã®ã€Œæœˆåˆ¥æŒ‡æ•°ã€éƒ¨åˆ†ã¯12å€‹ï¼ˆ4æœˆã€œ3æœˆï¼‰
        # ãŸã ã— nums ã«ã¯å¹´åº¦ç•ªå·ãªã©ãŒæ··ã–ã‚‹ã®ã§ã€æœ€å¾Œã®æ–¹ã«ã‚ã‚‹æ•°å€¤ç¾¤ã‚’å„ªå…ˆ
        # â†’ è¡Œæœ«å´ã‹ã‚‰æœ€å¤§12å€‹å–ã‚‹
        month_vals = list(map(int, nums[-12:]))

        months = list(range(4, 13)) + [1, 2, 3]
        years = [start_year]*9 + [start_year+1]*3

        for y, mo, v in zip(years, months, month_vals):
            data_points.append((pd.Timestamp(y, mo, 1), v))

    if not data_points:
        return pd.Series(dtype="float64")

    s = pd.Series({d: v for d, v in data_points}).sort_index()
    s = s[~s.index.duplicated(keep="last")]
    return s

# ----------------------------
# e-Stat: getStatsList -> pick statsDataId -> getStatsData
# ----------------------------
@st.cache_data(ttl=60 * 60)
def estat_get_stats_list(search_word: str, stats_code: str = "00450071", limit: int = 100) -> list[dict]:
    # ä»•æ§˜: çµ±è¨ˆè¡¨æƒ…å ±å–å¾—ï¼ˆgetStatsListï¼‰ :contentReference[oaicite:3]{index=3}
    url = "https://api.e-stat.go.jp/rest/3.0/app/json/getStatsList"
    params = {
        "appId": ESTAT_APP_ID,
        "searchWord": search_word,
        "statsCode": stats_code,
        "limit": limit,
        "lang": "J",
    }
    r = requests.get(url, params=params, timeout=60)
    r.raise_for_status()
    data = r.json()

    result = data.get("GET_STATS_LIST", {}).get("DATALIST_INF", {})
    tables = result.get("TABLE_INF", [])
    if isinstance(tables, dict):
        tables = [tables]

    out = []
    for t in tables:
        out.append({
            "statsDataId": t.get("@id"),
            "title": t.get("TITLE", ""),
            "updated": t.get("UPDATED_DATE", ""),
        })
    return [x for x in out if x["statsDataId"]]

@st.cache_data(ttl=60 * 60)
def estat_get_stats_data(stats_data_id: str, limit: int = 100000) -> dict:
    url = "https://api.e-stat.go.jp/rest/3.0/app/json/getStatsData"
    params = {
        "appId": ESTAT_APP_ID,
        "statsDataId": stats_data_id,
        "metaGetFlg": "Y",
        "cntGetFlg": "N",
        "limit": limit,
        "lang": "J",
    }
    r = requests.get(url, params=params, timeout=60)
    r.raise_for_status()
    return r.json()

def estat_build_class_maps(root: dict) -> dict:
    class_obj = root["CLASS_INF"]["CLASS_OBJ"]

    def to_map(obj):
        cls = obj["CLASS"]
        if isinstance(cls, dict):
            cls = [cls]
        return {c["@code"]: c["@name"] for c in cls}

    return {obj["@id"]: to_map(obj) for obj in class_obj}

def estat_series_from_statsdata(json_data: dict, *, industry_contains: str, item_contains: str) -> pd.Series:
    gsd = json_data.get("GET_STATS_DATA", {})
    stat = gsd.get("STATISTICAL_DATA")
    if not stat:
        return pd.Series(dtype="float64")

    class_maps = estat_build_class_maps(stat)
    values = stat["DATA_INF"]["VALUE"]
    if isinstance(values, dict):
        values = [values]

    # ã€Œã©ã®catãŒä½•ã‹ã€ã¯çµ±è¨ˆè¡¨ã”ã¨ã«å¤‰ã‚ã‚‹ã®ã§ã€åå‰ã§ã‚†ã‚‹ãæ¢ã™
    # â†’ class_maps ã®æ—¥æœ¬èªãƒ©ãƒ™ãƒ«ã« "è£½é€ æ¥­" ã‚„ "ç¾é‡‘çµ¦ä¸" ã‚’å«ã‚€ã‚³ãƒ¼ãƒ‰ã‚’é›†ã‚ã‚‹
    industry_codes = set()
    item_codes = set()

    for dim_id, cmap in class_maps.items():
        for code, name in cmap.items():
            if industry_contains in name:
                industry_codes.add(code)
            if item_contains in name:
                item_codes.add(code)

    rows = []
    for v in values:
        # time ã¯ "@time" ãŒåŸºæœ¬
        t = v.get("@time")
        val = v.get("$")
        if t is None or val is None:
            continue

        dim_codes = [v[k] for k in v.keys() if k.startswith("@cat")]
        if industry_codes and (not any(c in industry_codes for c in dim_codes)):
            continue
        if item_codes and (not any(c in item_codes for c in dim_codes)):
            continue

        try:
            rows.append((t, float(val)))
        except:
            continue

    if not rows:
        return pd.Series(dtype="float64")

    # time ã¯ "202501" ã‚„ "2025-01" ç­‰ãŒæ··ã–ã‚‹ã“ã¨ãŒã‚ã‚‹ã®ã§ä¸¡å¯¾å¿œ
    idx = []
    vals = []
    for t, v in rows:
        t_str = str(t)
        dt = None
        for fmt in ("%Y%m", "%Y-%m", "%Y-%m-%d"):
            try:
                dt = datetime.strptime(t_str, fmt)
                break
            except:
                pass
        if dt is None:
            # æœ€å¾Œã®æ‰‹æ®µ
            try:
                dt = pd.to_datetime(t_str)
            except:
                continue
        idx.append(pd.Timestamp(dt.year, dt.month, 1))
        vals.append(v)

    s = pd.Series(vals, index=idx).sort_index()
    s = s[~s.index.duplicated(keep="last")]
    return s

# ----------------------------
# Common plot helper
# ----------------------------
def plot_with_latest_highlight(series: pd.Series, title: str, y_label: str):
    if series.empty:
        st.info("ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚")
        return

    fig, ax = plt.subplots()
    ax.plot(series.index, series.values)
    ax.scatter(series.index[-1], series.values[-1], s=80, zorder=3)
    ax.annotate(
        f"{series.values[-1]:,.0f}",
        (series.index[-1], series.values[-1]),
        textcoords="offset points",
        xytext=(8, 8),
        ha="left",
    )
    ax.set_title(title)
    ax.set_xlabel("Month")
    ax.set_ylabel(y_label)
    ax.grid(True, alpha=0.3)
    st.pyplot(fig, clear_figure=True)

# ----------------------------
# 1) Copper / Aluminum (FRED)
# ----------------------------
copper = fetch_fred("PCOPPUSDM")
aluminum = fetch_fred("PALUMUSDM")
usdjpy = fetch_fred("EXJPUS")

df = pd.concat([copper, aluminum, usdjpy], axis=1, join="inner")
df.columns = ["copper_usd_ton", "aluminum_usd_ton", "usdjpy"]
df["copper_jpy_kg"] = df["copper_usd_ton"] * df["usdjpy"] / 1000
df["aluminum_jpy_kg"] = df["aluminum_usd_ton"] * df["usdjpy"] / 1000

latest_date = df.index[-1]
latest_month_str = latest_date.strftime("%Y-%m")

latest_copper = float(df.loc[latest_date, "copper_jpy_kg"])
latest_aluminum = float(df.loc[latest_date, "aluminum_jpy_kg"])

delta_copper = None
delta_aluminum = None
if len(df) >= 2:
    prev_date = df.index[-2]
    delta_copper = latest_copper - float(df.loc[prev_date, "copper_jpy_kg"])
    delta_aluminum = latest_aluminum - float(df.loc[prev_date, "aluminum_jpy_kg"])

st.subheader("ğŸ“Œ æœ€æ–°æœˆã®åŸä¾¡ï¼ˆå††/kgï¼‰")
k1, k2, k3 = st.columns([1, 1, 1])
with k1:
    st.metric(
        label=f"éŠ…ï¼ˆ{latest_month_str}ï¼‰",
        value=f"{latest_copper:,.0f} å††/kg",
        delta=f"{delta_copper:+,.0f} å††/kg" if delta_copper is not None else None,
    )
with k2:
    st.metric(
        label=f"ã‚¢ãƒ«ãƒŸï¼ˆ{latest_month_str}ï¼‰",
        value=f"{latest_aluminum:,.0f} å††/kg",
        delta=f"{delta_aluminum:+,.0f} å††/kg" if delta_aluminum is not None else None,
    )
with k3:
    st.metric(label="æ›´æ–°æ—¥æ™‚", value=datetime.now().strftime("%Y-%m-%d %H:%M"))

tab1, tab2, tab3 = st.tabs(["ğŸŸ  éŠ…", "âšª ã‚¢ãƒ«ãƒŸ", "ğŸ“‰ ã¾ã¨ã‚ï¼ˆåŒä¸€ã‚°ãƒ©ãƒ•ï¼‰"])
with tab1:
    st.subheader("éŠ… ä¾¡æ ¼æ¨ç§»ï¼ˆå††/kgï¼‰")
    plot_with_latest_highlight(df["copper_jpy_kg"], "Copper (JPY/kg)", "JPY/kg")
with tab2:
    st.subheader("ã‚¢ãƒ«ãƒŸ ä¾¡æ ¼æ¨ç§»ï¼ˆå††/kgï¼‰")
    plot_with_latest_highlight(df["aluminum_jpy_kg"], "Aluminum (JPY/kg)", "JPY/kg")
with tab3:
    st.subheader("éŠ…ãƒ»ã‚¢ãƒ«ãƒŸï¼ˆå††/kgï¼‰åŒä¸€ã‚°ãƒ©ãƒ•")
    st.line_chart(df[["copper_jpy_kg", "aluminum_jpy_kg"]])

st.divider()

# ----------------------------
# 2) WebKIT freight index
# ----------------------------
st.subheader("ğŸšš å›½å†…ãƒˆãƒ©ãƒƒã‚¯é‹è³ƒæŒ‡æ•°ï¼ˆWebKIT æˆç´„é‹è³ƒæŒ‡æ•°ï¼‰")

try:
    pdf_url = webkit_latest_pdf_url()
    st.caption(f"æœ€æ–°PDF: {pdf_url}")
    webkit = fetch_webkit_index_from_pdf(pdf_url)

    if webkit.empty:
        st.warning("PDFã‹ã‚‰æœˆåˆ¥æŒ‡æ•°ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸï¼ˆPDFæ§‹é€ ãŒå¤‰ã‚ã£ãŸå¯èƒ½æ€§ï¼‰ã€‚")
    else:
        plot_with_latest_highlight(webkit, "WebKIT Freight Index", "Index (2010-04=100)")
        st.line_chart(webkit.rename("webkit_freight_index"))
except Exception as e:
    st.error(f"WebKITå–å¾—ã§ã‚¨ãƒ©ãƒ¼: {e}")

st.divider()

# ----------------------------
# 3) e-Stat wage (manufacturing)
# ----------------------------
st.subheader("ğŸ’´ è£½é€ æ¥­ã®è³ƒé‡‘ï¼ˆe-Stat APIï¼‰")

if not ESTAT_APP_ID:
    st.info("ESTAT_APP_ID ãŒæœªè¨­å®šã®ãŸã‚ã€ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯åœæ­¢ã—ã¦ã„ã¾ã™ã€‚")
else:
    # ã¾ãšçµ±è¨ˆè¡¨ã‚’æ¤œç´¢ï¼ˆstatsDataIdã‚’è‡ªå‹•ã§å€™è£œã«ã™ã‚‹ï¼‰
    search_word = st.text_input(
        "çµ±è¨ˆè¡¨ã®æ¤œç´¢ãƒ¯ãƒ¼ãƒ‰ï¼ˆä¾‹ï¼‰",
        value="æ¯æœˆå‹¤åŠ´çµ±è¨ˆèª¿æŸ» å…¨å›½èª¿æŸ» ç¾é‡‘çµ¦ä¸ç·é¡",
        help="ã“ã“ã§å€™è£œã®çµ±è¨ˆè¡¨ï¼ˆstatsDataIdï¼‰ã‚’æ¤œç´¢ã—ã¾ã™ã€‚",
    )

    try:
        tables = estat_get_stats_list(search_word=search_word, stats_code="00450071", limit=100)

        if not tables:
            st.warning("å€™è£œãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚æ¤œç´¢ãƒ¯ãƒ¼ãƒ‰ã‚’å¤‰ãˆã¦ãã ã•ã„ã€‚")
        else:
            # ã‚¿ã‚¤ãƒˆãƒ«ã‚’è¦‹ãªãŒã‚‰é¸ã¹ã‚‹ã‚ˆã†ã«ã™ã‚‹ï¼ˆã“ã‚ŒãŒä¸€ç•ªç¢ºå®Ÿï¼‰
            options = {f'{t["statsDataId"]} | {t["title"]}': t["statsDataId"] for t in tables}
            selected_key = st.selectbox("å–å¾—ã™ã‚‹çµ±è¨ˆè¡¨ã‚’é¸æŠï¼ˆstatsDataIdï¼‰", list(options.keys()))
            stats_data_id = options[selected_key]

            estat_json = estat_get_stats_data(stats_data_id)

            # â€œè£½é€ æ¥­â€ Ã— â€œç¾é‡‘çµ¦ä¸ç·é¡â€ ã‚’ã‚†ã‚‹ãæŠ½å‡º
            wage_mfg = estat_series_from_statsdata(
                estat_json,
                industry_contains="è£½é€ æ¥­",
                item_contains="ç¾é‡‘çµ¦ä¸ç·é¡",
            )

            if wage_mfg.empty:
                st.warning("è£½é€ æ¥­Ã—ç¾é‡‘çµ¦ä¸ç·é¡ã®ç³»åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                st.info("ãƒ’ãƒ³ãƒˆï¼šitem_contains ã‚’ 'ãã¾ã£ã¦æ”¯çµ¦ã™ã‚‹çµ¦ä¸' ãªã©ã«å¤‰ãˆã‚‹ã¨è¦‹ã¤ã‹ã‚‹è¡¨ã‚‚ã‚ã‚Šã¾ã™ã€‚")
            else:
                plot_with_latest_highlight(wage_mfg, "Manufacturing Wage (e-Stat)", "Value")
                st.line_chart(wage_mfg.rename("wage_mfg"))

    except Exception as e:
        st.error(f"e-Statå–å¾—ã§ã‚¨ãƒ©ãƒ¼: {e}")

def _df_to_sheet(ws, df: pd.DataFrame, start_row=1, start_col=1):
    for r_idx, row in enumerate(dataframe_to_rows(df, index=False, header=True), start_row):
        for c_idx, v in enumerate(row, start_col):
            ws.cell(row=r_idx, column=c_idx, value=v)

def _save_series_chart_png(series: pd.Series, title: str, y_label: str, path: str):
    fig, ax = plt.subplots(figsize=(10, 4))
    ax.plot(series.index, series.values)
    ax.scatter(series.index[-1], series.values[-1], s=120, zorder=3)
    ax.set_title(title)
    ax.set_xlabel("Month")
    ax.set_ylabel(y_label)
    ax.grid(True, alpha=0.3)
    fig.tight_layout()
    fig.savefig(path, dpi=200)
    plt.close(fig)

def build_monthly_master(df_fred: pd.DataFrame, webkit: pd.Series, wage: pd.Series) -> pd.DataFrame:
    """
    df_fred: index=date, columns=[copper_jpy_kg, aluminum_jpy_kg] ã‚’æƒ³å®š
    webkit, wage: index=date (æœˆåˆ) ã® Series
    """
    out = df_fred.copy()

    # æœˆæ¬¡indexã¸å¯„ã›ã‚‹ï¼ˆFREDã¯æ—¥ä»˜ãŒæœˆæœ«/ç‰¹å®šæ—¥ã«ãªã‚‹ã“ã¨ãŒã‚ã‚‹ã®ã§æœˆåˆã«çµ±ä¸€ï¼‰
    out = out.copy()
    out.index = pd.to_datetime(out.index)
    out.index = out.index.to_period("M").to_timestamp("MS")

    if webkit is not None and not webkit.empty:
        s = webkit.copy()
        s.index = pd.to_datetime(s.index).to_period("M").to_timestamp("MS")
        out["webkit_freight_index"] = s

    if wage is not None and not wage.empty:
        s = wage.copy()
        s.index = pd.to_datetime(s.index).to_period("M").to_timestamp("MS")
        out["wage_mfg"] = s

    out = out.sort_index()
    return out

def make_excel_report(master: pd.DataFrame) -> bytes:
    """
    master: æœˆæ¬¡ã®çµ±åˆãƒ‡ãƒ¼ã‚¿
    è¿”ã‚Šå€¤: xlsxã®ãƒã‚¤ãƒŠãƒªï¼ˆst.download_button ã«æ¸¡ã›ã‚‹ï¼‰
    """
    wb = Workbook()
    ws_sum = wb.active
    ws_sum.title = "Summary"
    ws_data = wb.create_sheet("Data")
    ws_charts = wb.create_sheet("Charts")

    # --- Summaryï¼ˆæœ€æ–°å€¤ã¨å‰æœˆå·®ï¼‰
    latest = master.dropna(how="all").iloc[-1]
    latest_month = master.dropna(how="all").index[-1].strftime("%Y-%m")

    ws_sum["A1"] = "æœ€æ–°æœˆ"
    ws_sum["B1"] = latest_month
    ws_sum["A3"] = "æŒ‡æ¨™"
    ws_sum["B3"] = "æœ€æ–°å€¤"
    ws_sum["C3"] = "å‰æœˆå·®"

    # å‰æœˆå·®ï¼ˆåˆ—ã”ã¨ï¼‰
    rows = []
    for col in master.columns:
        s = master[col].dropna()
        if s.empty:
            continue
        v_now = float(s.iloc[-1])
        v_prev = float(s.iloc[-2]) if len(s) >= 2 else None
        delta = v_now - v_prev if v_prev is not None else None
        rows.append((col, v_now, delta))

    for i, (name, v, d) in enumerate(rows, start=4):
        ws_sum[f"A{i}"] = name
        ws_sum[f"B{i}"] = v
        ws_sum[f"C{i}"] = d

    ws_sum["A14"] = "è¦å› ï¼ˆãƒ¡ãƒ¢ï¼‰"
    ws_sum["A15"] = "ãƒ»ï¼ˆã“ã“ã¯å¾Œã§LLMã§è‡ªå‹•ç”Ÿæˆã—ã¦åŸ‹ã‚ã‚‹ã®ãŒä¸€ç•ªä¾¡å€¤å‡ºã‚‹ï¼‰"

    # --- Dataã‚·ãƒ¼ãƒˆï¼ˆè¡¨ï¼‰
    export = master.copy()
    export = export.reset_index().rename(columns={"index": "month"})
    export["month"] = pd.to_datetime(export["month"]).dt.strftime("%Y-%m")
    _df_to_sheet(ws_data, export, start_row=1, start_col=1)

    # --- Chartsã‚·ãƒ¼ãƒˆï¼ˆç”»åƒè²¼ã‚Šä»˜ã‘ï¼‰
    # openpyxl ã¯ç”»åƒã‚’ã€Œãƒ•ã‚¡ã‚¤ãƒ«ã€çµŒç”±ã§è²¼ã‚‹ã®ãŒå®‰å®šãªã®ã§ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ã†
    with tempfile.TemporaryDirectory() as td:
        # ãã‚Œãã‚Œå­˜åœ¨ã™ã‚‹åˆ—ã ã‘å‡ºã™
        chart_specs = []
        if "copper_jpy_kg" in master.columns:
            chart_specs.append(("copper_jpy_kg", "Copper (JPY/kg)", "JPY/kg"))
        if "aluminum_jpy_kg" in master.columns:
            chart_specs.append(("aluminum_jpy_kg", "Aluminum (JPY/kg)", "JPY/kg"))
        if "webkit_freight_index" in master.columns:
            chart_specs.append(("webkit_freight_index", "WebKIT Freight Index", "Index"))
        if "wage_mfg" in master.columns:
            chart_specs.append(("wage_mfg", "Manufacturing Wage", "Value"))

        anchor_row = 1
        for col, title, ylab in chart_specs:
            s = master[col].dropna()
            if s.empty:
                continue

            png = f"{td}/{col}.png"
            _save_series_chart_png(s, title, ylab, png)

            img = XLImage(png)
            img.anchor = f"A{anchor_row}"
            ws_charts.add_image(img)

            # æ¬¡ã®ç”»åƒã®è²¼ã‚Šä»˜ã‘ä½ç½®ï¼ˆã–ã£ãã‚Šä¸‹ã¸ï¼‰
            anchor_row += 22

    # --- ãƒã‚¤ãƒŠãƒªåŒ–ã—ã¦è¿”ã™
    bio = io.BytesIO()
    wb.save(bio)
    return bio.getvalue()
st.divider()
st.subheader("ğŸ“¦ Excelãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›")

# FREDç”±æ¥ã®ãƒ™ãƒ¼ã‚¹ï¼ˆå††/kgåˆ—ã ã‘ã«ã™ã‚‹ï¼‰
df_fred = df[["copper_jpy_kg", "aluminum_jpy_kg"]].copy()

# webkit / wage_mfg ãŒæœªå®šç¾©ã®ã‚±ãƒ¼ã‚¹ã‚’å¸å
try:
    webkit_for_export = webkit
except NameError:
    webkit_for_export = pd.Series(dtype="float64")

try:
    wage_for_export = wage_mfg
except NameError:
    wage_for_export = pd.Series(dtype="float64")

master = build_monthly_master(df_fred, webkit_for_export, wage_for_export)

xlsx_bytes = make_excel_report(master)

st.download_button(
    label="â¬‡ï¸ Excelãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
    data=xlsx_bytes,
    file_name=f"cost_report_{datetime.now().strftime('%Y%m%d_%H%M')}.xlsx",
    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
)



