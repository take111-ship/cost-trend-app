import requests
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt
from datetime import date, datetime

# ----------------------------
# Page / UI
# ----------------------------
st.set_page_config(
    page_title="éŠ…ãƒ»ã‚¢ãƒ«ãƒŸ åŸä¾¡ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰",
    page_icon="ğŸ“ˆ",
    layout="wide",
)

st.title("ğŸ“ˆ éŠ…ãƒ»ã‚¢ãƒ«ãƒŸ åŸä¾¡æŒ‡æ¨™ï¼ˆå††/kgãƒ»æœˆæ¬¡ï¼‰")
st.caption("FREDã®æœˆæ¬¡ãƒ‡ãƒ¼ã‚¿ï¼ˆUSD/tonï¼‰Ã— USDJPY ã‹ã‚‰å††/kgã«æ›ç®—ã—ã¦è¡¨ç¤ºã—ã¾ã™ã€‚")
with st.sidebar:
    st.header("ğŸ“š ãƒ‡ãƒ¼ã‚¿å…ƒ / è¨ˆç®—æ¡ä»¶")

    st.markdown("""
    **â–  ãƒ‡ãƒ¼ã‚¿å…ƒï¼ˆFREDï¼‰**  
    - éŠ…ä¾¡æ ¼ï¼ˆUSD/tonï¼‰  
      https://fred.stlouisfed.org/series/PCOPPUSDM  
    - ã‚¢ãƒ«ãƒŸä¾¡æ ¼ï¼ˆUSD/tonï¼‰  
      https://fred.stlouisfed.org/series/PALUMUSDM  
    - ç‚ºæ›¿ï¼ˆUSD/JPYï¼‰  
      https://fred.stlouisfed.org/series/EXJPUS  

    ---

    **â–  è¨ˆç®—å¼**  
    USD/ton Ã— USD/JPY Ã· 1000 = å††/kg  

    ---

    **â–  æ›´æ–°é »åº¦**  
    æœˆæ¬¡ï¼ˆFREDã®æ›´æ–°ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã«ä¾å­˜ï¼‰
    """)


with st.expander("ã“ã®ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã«ã¤ã„ã¦", expanded=True):
    st.markdown(
        """
- **ç›®çš„**ï¼šåŸææ–™ï¼ˆéŠ…ãƒ»ã‚¢ãƒ«ãƒŸï¼‰ã®åŸä¾¡æ„Ÿã‚’â€œæœˆæ¬¡ã§â€ã¤ã‹ã‚€  
- **è¨ˆç®—**ï¼šUSD/ton Ã— USDJPY Ã· 1000 = **å††/kg**  
- **ãƒ‡ãƒ¼ã‚¿**ï¼šFREDï¼ˆPCOPPUSDM / PALUMUSDM / EXJPUSï¼‰  
- **è¦‹æ–¹**ï¼šã‚°ãƒ©ãƒ•ã¯ **æœ€æ–°æœˆã‚’å¼·èª¿è¡¨ç¤º**ã€KPIã¯ **å‰æœˆæ¯”** ã¤ã  
"""
    )


# ----------------------------
# Secrets / API key
# ----------------------------
FRED_API_KEY = st.secrets.get("FRED_API_KEY", "")
if not FRED_API_KEY:
    st.error("FRED_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼ˆStreamlit Secretsã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼‰")
    st.stop()

# ----------------------------
# Data fetch
# ----------------------------
@st.cache_data(ttl=60 * 60)  # 1æ™‚é–“ã‚­ãƒ£ãƒƒã‚·ãƒ¥
def fetch(series_id: str, start: str = "2018-01-01") -> pd.Series:
    url = "https://api.stlouisfed.org/fred/series/observations"
    params = {
        "series_id": series_id,
        "api_key": FRED_API_KEY,
        "file_type": "json",
        "observation_start": start,
        "observation_end": date.today().isoformat(),
    }
    r = requests.get(url, params=params, timeout=30)
    r.raise_for_status()
    obs = r.json()["observations"]

    df = pd.DataFrame(obs)[["date", "value"]]
    df["date"] = pd.to_datetime(df["date"])
    df["value"] = pd.to_numeric(df["value"], errors="coerce")
    s = df.dropna().set_index("date")["value"].sort_index()
    return s

copper = fetch("PCOPPUSDM")
aluminum = fetch("PALUMUSDM")
usdjpy = fetch("EXJPUS")

df = pd.concat([copper, aluminum, usdjpy], axis=1, join="inner")
df.columns = ["copper_usd_ton", "aluminum_usd_ton", "usdjpy"]

df["copper_jpy_kg"] = df["copper_usd_ton"] * df["usdjpy"] / 1000
df["aluminum_jpy_kg"] = df["aluminum_usd_ton"] * df["usdjpy"] / 1000

@st.cache_data(ttl=60*60)
def fetch_estat(stats_data_id: str, params_extra: dict) -> pd.Series:
    app_id = st.secrets.get("ESTAT_APP_ID", "")
    if not app_id:
        st.error("ESTAT_APP_ID ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼ˆStreamlit Secretsã‚’ç¢ºèªï¼‰")
        st.stop()

    url = "https://api.e-stat.go.jp/rest/3.0/app/json/getStatsData"
    params = {
        "appId": app_id,
        "statsDataId": stats_data_id,
        # "cdCat01": "...",  # æŒ‡æ¨™ï¼ˆç¾é‡‘çµ¦ä¸ç·é¡ãªã©ï¼‰
        # "cdCat02": "...",  # ç”£æ¥­ï¼ˆè£½é€ æ¥­ï¼‰
        # "metaGetFlg": "N",
        "lang": "J",
    }
    params.update(params_extra)

    r = requests.get(url, params=params, timeout=30)
    r.raise_for_status()
    data = r.json()

    # e-Statã¯æ§‹é€ ãŒå°‘ã—è¤‡é›‘ãªã®ã§ã€ã¾ãšã¯å€¤ã®å…¥ã£ã¦ã„ã‚‹é…åˆ—ã‚’å–ã‚Šå‡ºã™
    values = data["GET_STATS_DATA"]["STATISTICAL_DATA"]["DATA_INF"]["VALUE"]

    df = pd.DataFrame(values)
    # df["$"] ãŒæ•°å€¤ã€æ™‚é–“ã‚³ãƒ¼ãƒ‰ãŒ "@time" ãªã©ã§å…¥ã‚‹ï¼ˆè¡¨ã«ã‚ˆã‚Šå¤‰ã‚ã‚‹ï¼‰
    # ã“ã“ã¯ã€Œã‚ãªãŸãŒé¸ã‚“ã è¡¨ã€ã«åˆã‚ã›ã¦åˆ—åã‚’èª¿æ•´ã™ã‚‹
    df["value"] = pd.to_numeric(df["$"], errors="coerce")

    # æ™‚é–“ã‚­ãƒ¼ã®å€™è£œï¼ˆè¡¨ã«ã‚ˆã‚Šé•ã†ã®ã§é †ã«è©¦ã™ï¼‰
    time_key = None
    for k in ["@time", "@TIME", "@cat03", "@cat01"]:
        if k in df.columns:
            time_key = k
            break
    if time_key is None:
        raise ValueError("e-Statã®æ™‚é–“ã‚­ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚df.columns ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

    # æœˆæ¬¡ã«ã™ã‚‹ï¼ˆYYYYMM å½¢å¼ãŒå¤šã„ï¼‰
    df["date"] = pd.to_datetime(df[time_key].astype(str), format="%Y%m", errors="coerce")
    s = df.dropna(subset=["date", "value"]).set_index("date")["value"].sort_index()
    return s


# ----------------------------
# Latest / KPI
# ----------------------------
latest_date = df.index[-1]
latest_month_str = latest_date.strftime("%Y-%m")

latest_copper = float(df.loc[latest_date, "copper_jpy_kg"])
latest_aluminum = float(df.loc[latest_date, "aluminum_jpy_kg"])

# å‰æœˆæ¯”ï¼ˆå·®åˆ†ï¼‰
delta_copper = None
delta_aluminum = None
if len(df) >= 2:
    prev_date = df.index[-2]
    delta_copper = latest_copper - float(df.loc[prev_date, "copper_jpy_kg"])
    delta_aluminum = latest_aluminum - float(df.loc[prev_date, "aluminum_jpy_kg"])

st.subheader("ğŸ“Œ æœ€æ–°æœˆã®åŸä¾¡ï¼ˆå††/kgï¼‰")

k1, k2, k3 = st.columns([1, 1, 1])
with k1:
    st.metric(
        label=f"éŠ…ï¼ˆ{latest_month_str}ï¼‰",
        value=f"{latest_copper:,.0f} å††/kg",
        delta=f"{delta_copper:+,.0f} å††/kg" if delta_copper is not None else None,
    )
with k2:
    st.metric(
        label=f"ã‚¢ãƒ«ãƒŸï¼ˆ{latest_month_str}ï¼‰",
        value=f"{latest_aluminum:,.0f} å††/kg",
        delta=f"{delta_aluminum:+,.0f} å††/kg" if delta_aluminum is not None else None,
    )
with k3:
    st.metric(
        label="æ›´æ–°æ—¥æ™‚",
        value=datetime.now().strftime("%Y-%m-%d %H:%M"),
    )

st.divider()

# ----------------------------
# Charts (tabs) + highlight latest point
# ----------------------------
def plot_with_latest_highlight(series: pd.Series, title: str, y_label: str):
    fig, ax = plt.subplots()
    ax.plot(series.index, series.values)

    # æœ€æ–°ç‚¹ã‚’å¼·èª¿
    ax.scatter(series.index[-1], series.values[-1], s=80, zorder=3)
    ax.annotate(
        f"{series.values[-1]:,.0f}",
        (series.index[-1], series.values[-1]),
        textcoords="offset points",
        xytext=(8, 8),
        ha="left",
    )

    ax.set_title(title)
    ax.set_xlabel("Month")
    ax.set_ylabel(y_label)
    ax.grid(True, alpha=0.3)
    st.pyplot(fig, clear_figure=True)

tab1, tab2, tab3 = st.tabs(["ğŸŸ  éŠ…", "âšª ã‚¢ãƒ«ãƒŸ", "ğŸ“‰ ã¾ã¨ã‚ï¼ˆåŒä¸€ã‚°ãƒ©ãƒ•ï¼‰"])

with tab1:
    st.subheader("éŠ… ä¾¡æ ¼æ¨ç§»ï¼ˆå††/kgï¼‰")
    plot_with_latest_highlight(df["copper_jpy_kg"], "Copper (JPY/kg)", "JPY/kg")
    st.caption("ãƒ‡ãƒ¼ã‚¿ï¼šFRED PCOPPUSDMï¼ˆUSD/tonï¼‰ã¨ EXJPUSï¼ˆUSDJPYï¼‰ã‹ã‚‰æ›ç®—")

with tab2:
    st.subheader("ã‚¢ãƒ«ãƒŸ ä¾¡æ ¼æ¨ç§»ï¼ˆå††/kgï¼‰")
    plot_with_latest_highlight(df["aluminum_jpy_kg"], "Aluminum (JPY/kg)", "JPY/kg")
    st.caption("ãƒ‡ãƒ¼ã‚¿ï¼šFRED PALUMUSDMï¼ˆUSD/tonï¼‰ã¨ EXJPUSï¼ˆUSDJPYï¼‰ã‹ã‚‰æ›ç®—")

with tab3:
    st.subheader("éŠ…ãƒ»ã‚¢ãƒ«ãƒŸï¼ˆå††/kgï¼‰åŒä¸€ã‚°ãƒ©ãƒ•")
    st.line_chart(df[["copper_jpy_kg", "aluminum_jpy_kg"]])
    st.caption("ã–ã£ãã‚Šæ¯”è¼ƒã—ãŸã„äººå‘ã‘ï¼ˆè©³ç´°ã¯å„ã‚¿ãƒ–ã¸ï¼‰")

st.divider()

# ----------------------------
# Data table + download
# ----------------------------
st.subheader("ğŸ“„ ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼‰")

download_df = df[["copper_jpy_kg", "aluminum_jpy_kg"]].copy()
download_df = download_df.rename(
    columns={
        "copper_jpy_kg": "copper_jpy_per_kg",
        "aluminum_jpy_kg": "aluminum_jpy_per_kg",
    }
)
download_df.index.name = "date"

with st.expander("è¡¨ã‚’è¡¨ç¤º"):
    st.dataframe(download_df.tail(24), use_container_width=True)

csv_bytes = download_df.to_csv(encoding="utf-8-sig").encode("utf-8-sig")

st.download_button(
    label="ğŸ“¥ CSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
    data=csv_bytes,
    file_name="copper_aluminum_jpy_per_kg_monthly.csv",
    mime="text/csv",
)

import re

E_STAT_APP_ID = st.secrets.get("E_STAT_APP_ID", "")
if not E_STAT_APP_ID:
    st.error("E_STAT_APP_ID ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼ˆStreamlit Secretsã«è¿½åŠ ã—ã¦ãã ã•ã„ï¼‰")
    st.stop()

def fetch_estat_statsdata(stats_data_id: str, limit: int = 100000):
    # e-Stat: getStatsData (JSON)
    url = "https://api.e-stat.go.jp/rest/3.0/app/json/getStatsData"
    params = {
        "appId": E_STAT_APP_ID,
        "statsDataId": stats_data_id,
        "metaGetFlg": "Y",   # ãƒ¡ã‚¿æƒ…å ±ã‚‚ä¸€ç·’ã«å–ã‚‹ï¼ˆã‚³ãƒ¼ãƒ‰â†’æ—¥æœ¬èªåã®è¾æ›¸ã«ä½¿ã†ï¼‰
        "cntGetFlg": "N",
        "limit": limit,
    }
    r = requests.get(url, params=params, timeout=60)
    r.raise_for_status()
    return r.json()

def estat_pick_series(json_data, industry_label_contains="è£½é€ æ¥­", item_label_contains="è³ƒé‡‘æŒ‡æ•°"):
    """
    è¿”ã£ã¦ããŸJSONã‹ã‚‰ã€
    - ç”£æ¥­åˆ†é¡ã§ã€Œè£½é€ æ¥­ã€
    - è¡¨ç« é …ç›®ã§ã€Œè³ƒé‡‘æŒ‡æ•°ï¼ˆç¾é‡‘çµ¦ä¸ç·é¡ï¼‰ã€ç³»
    ã‚’ã–ã£ãã‚Šé¸ã‚“ã§ã€æ™‚ç³»åˆ—(series)ã«ã™ã‚‹ã€‚
    """
    root = json_data["GET_STATS_DATA"]["STATISTICAL_DATA"]
    class_inf = root["CLASS_INF"]["CLASS_OBJ"]
    values = root["DATA_INF"]["VALUE"]

    # CLASS_OBJ ã‚’ nameâ†’{code:label} ã«æ•´å½¢
    def to_map(obj):
        # obj["CLASS"] ã¯ list ã ã£ãŸã‚Š dict ã ã£ãŸã‚Šã™ã‚‹
        cls = obj["CLASS"]
        if isinstance(cls, dict):
            cls = [cls]
        return {c["@code"]: c["@name"] for c in cls}

    class_maps = {obj["@id"]: to_map(obj) for obj in class_inf}

    # ã©ã®æ¬¡å…ƒãŒã€Œç”£æ¥­ã€ã€Œè¡¨ç« é …ç›®ã€ã‹ã¯çµ±è¨ˆè¡¨ã«ã‚ˆã£ã¦é•ã†ã®ã§
    # VALUEã®ä¸­ã®ã‚­ãƒ¼ï¼ˆä¾‹ï¼š@cat01, @cat02...ï¼‰ã‚’è¦‹ã¦ç·å½“ãŸã‚Šæ°—å‘³ã«æ¢ã™
    # ã¾ãšã€Œè£½é€ æ¥­ã€ã¨ã„ã†ãƒ©ãƒ™ãƒ«ã‚’å«ã‚€ã‚³ãƒ¼ãƒ‰å€™è£œã‚’å…¨éƒ¨é›†ã‚ã‚‹
    industry_codes = set()
    item_codes = set()
    for dim_id, cmap in class_maps.items():
        for code, name in cmap.items():
            if industry_label_contains in name:
                industry_codes.add(code)
            if item_label_contains in name:
                item_codes.add(code)

    # å®Ÿãƒ‡ãƒ¼ã‚¿ã‚’ãªã‚ã¦ã€è£½é€ æ¥­Ã—è³ƒé‡‘æŒ‡æ•°ã£ã½ã„ã‚‚ã®ã‚’æ‹¾ã†
    rows = []
    for v in values:
        time = v.get("@time")
        val = v.get("$")
        # å„æ¬¡å…ƒã®ã‚³ãƒ¼ãƒ‰ã‚’æ‹¾ã†ï¼ˆ@cat01, @cat02... ã®ã‚ˆã†ãªã‚‚ã®ï¼‰
        dim_codes = [v[k] for k in v.keys() if k.startswith("@cat")]
        if any(c in industry_codes for c in dim_codes) and (len(item_codes) == 0 or any(c in item_codes for c in dim_codes)):
            rows.append((time, float(val)))

    if not rows:
        return pd.Series(dtype="float64")

    s = pd.Series(
        data=[x[1] for x in rows],
        index=pd.to_datetime([x[0] for x in rows])
    ).sort_index()

    # åŒã˜æœˆãŒé‡è¤‡ã™ã‚‹ã“ã¨ãŒã‚ã‚‹ã®ã§ã€æœ€å¾Œã‚’æ¡ç”¨
    s = s[~s.index.duplicated(keep="last")]
    return s

# ---- ã“ã“ã§å®Ÿéš›ã«å–å¾—ï¼ˆä¾‹ã®statsDataIdï¼‰----
estat_json = fetch_estat_statsdata("000008232508")  # æ¯æœˆå‹¤åŠ´çµ±è¨ˆèª¿æŸ»ï¼ˆç”£æ¥­åˆ¥è³ƒé‡‘æŒ‡æ•°ã®ä¾‹ï¼‰:contentReference[oaicite:6]{index=6}
wage_mfg = estat_pick_series(estat_json, industry_label_contains="è£½é€ æ¥­", item_label_contains="è³ƒé‡‘æŒ‡æ•°")

st.subheader("è£½é€ æ¥­ã®è³ƒé‡‘æŒ‡æ¨™ï¼ˆe-Statï¼‰")
if wage_mfg.empty:
    st.warning("è£½é€ æ¥­ã®ç³»åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆè¡¨ã®é¸ã³æ–¹ã‚’èª¿æ•´ã—ã¾ã™ï¼‰")
else:
    st.line_chart(wage_mfg.rename("wage_index_mfg"))

import io
import pdfplumber

def fetch_webkit_index_from_pdf(pdf_url: str):
    r = requests.get(pdf_url, timeout=60)
    r.raise_for_status()

    with pdfplumber.open(io.BytesIO(r.content)) as pdf:
        text = "\n".join(page.extract_text() or "" for page in pdf.pages)

    # ã€Œæˆç´„é‹è³ƒæŒ‡æ•°ï¼ˆæœˆåˆ¥ï¼‰ã®æ¨ç§»ã€ãƒ†ãƒ¼ãƒ–ãƒ«å‘¨è¾ºã‚’ä½¿ã†
    # ä¾‹: "ä»¤å’Œï¼—å¹´åº¦ 137 135 131 135 143 138 137 141 146"
    # å¹´åº¦è¡Œã‚’æ‹¾ã£ã¦ã€å¹´åº¦â†’4æœˆã€œ3æœˆã®å€¤ã«å¤‰æ›
    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
    rows = [ln for ln in lines if ln.startswith("ä»¤å’Œ") or ln.startswith("å¹³æˆ")]

    data = []
    for ln in rows:
        # å¹´åº¦å + æ•°å­—ã‚’æ‹¾ã†
        nums = re.findall(r"\b\d+\b", ln)
        if len(nums) < 3:
            continue
        year_label = ln.split()[0]  # "ä»¤å’Œï¼—å¹´åº¦" ãªã©
        values = list(map(int, nums))  # 100 98 ... ã®éƒ¨åˆ†
        # 4æœˆã€œ3æœˆï¼ˆæœ€å¤§12å€‹ï¼‰ã¨ã—ã¦æ‰±ã†
        values = values[:12]
        data.append((year_label, values))

    # å¹´åº¦â†’æœˆã¸å±•é–‹ï¼ˆã–ã£ãã‚Šï¼šå¹´åº¦ã¯4æœˆã‚¹ã‚¿ãƒ¼ãƒˆï¼‰
    # "ä»¤å’Œï¼—å¹´åº¦" â†’ 2025å¹´åº¦ï¼ˆä»¤å’Œ7=2025ï¼‰ãªã®ã§ 2025-04ã€œ
    series = []
    for year_label, vals in data:
        m = re.search(r"(ä»¤å’Œ|å¹³æˆ)(\d+)å¹´åº¦", year_label)
        if not m:
            continue
        era, n = m.group(1), int(m.group(2))
        if era == "ä»¤å’Œ":
            start_year = 2018 + n  # ä»¤å’Œ1=2019 â†’ 2018+1
        else:
            start_year = 1988 + n  # å¹³æˆ1=1989 â†’ 1988+1
        # 4æœˆã€œ12æœˆï¼ˆ9å€‹ï¼‰+ 1æœˆã€œ3æœˆï¼ˆ3å€‹ï¼‰
        months = list(range(4, 13)) + [1, 2, 3]
        years = [start_year]*9 + [start_year+1]*3
        for y, mo, v in zip(years, months, vals):
            series.append((pd.Timestamp(y, mo, 1), v))

    s = pd.Series({d: v for d, v in series}).sort_index()
    s = s[~s.index.duplicated(keep="last")]
    return s

st.subheader("å›½å†…ãƒˆãƒ©ãƒƒã‚¯é‹è³ƒæŒ‡æ•°ï¼ˆWebKIT æˆç´„é‹è³ƒæŒ‡æ•°ï¼‰")
# æœ€æ–°ã®PDFï¼ˆä¾‹ï¼š2025å¹´12æœˆåˆ†ã®PDFï¼‰:contentReference[oaicite:9]{index=9}
webkit = fetch_webkit_index_from_pdf("https://jta.or.jp/pdf/kit_release/202512.pdf")
st.line_chart(webkit.rename("webkit_freight_index"))



