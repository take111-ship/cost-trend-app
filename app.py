import io
import re
import requests
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt
from datetime import date, datetime

# ----------------------------
# Page / UI
# ----------------------------
st.set_page_config(
    page_title="éŠ…ãƒ»ã‚¢ãƒ«ãƒŸ åŸä¾¡ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰",
    page_icon="ğŸ“ˆ",
    layout="wide",
)

st.title("ğŸ“ˆ éŠ…ãƒ»ã‚¢ãƒ«ãƒŸ åŸä¾¡æŒ‡æ¨™ï¼ˆå††/kgãƒ»æœˆæ¬¡ï¼‰")
st.caption("FREDã®æœˆæ¬¡ãƒ‡ãƒ¼ã‚¿ï¼ˆUSD/tonï¼‰Ã— USDJPY ã‹ã‚‰å††/kgã«æ›ç®—ã—ã¦è¡¨ç¤ºã—ã¾ã™ã€‚")

with st.sidebar:
    st.header("ğŸ“š ãƒ‡ãƒ¼ã‚¿å…ƒ / è¨ˆç®—æ¡ä»¶")
    st.markdown("""
**â–  ãƒ‡ãƒ¼ã‚¿å…ƒ**
- FRED  
  - éŠ…ï¼ˆPCOPPUSDMï¼‰ https://fred.stlouisfed.org/series/PCOPPUSDM  
  - ã‚¢ãƒ«ãƒŸï¼ˆPALUMUSDMï¼‰ https://fred.stlouisfed.org/series/PALUMUSDM  
  - ç‚ºæ›¿ USD/JPYï¼ˆEXJPUSï¼‰ https://fred.stlouisfed.org/series/EXJPUS  
- e-Statï¼ˆæ¯æœˆå‹¤åŠ´çµ±è¨ˆèª¿æŸ» ãªã©ï¼‰ https://www.e-stat.go.jp/
- å…¨æ—¥æœ¬ãƒˆãƒ©ãƒƒã‚¯å”ä¼š WebKITï¼ˆæˆç´„é‹è³ƒæŒ‡æ•°ï¼‰ https://jta.or.jp/

---

**â–  è¨ˆç®—å¼ï¼ˆéŠ…ãƒ»ã‚¢ãƒ«ãƒŸï¼‰**  
USD/ton Ã— USD/JPY Ã· 1000 = å††/kg  

---

**â–  æ›´æ–°é »åº¦**  
æœˆæ¬¡ï¼ˆå„ãƒ‡ãƒ¼ã‚¿æä¾›å…ƒã®æ›´æ–°ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã«ä¾å­˜ï¼‰
""")

with st.expander("ã“ã®ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã«ã¤ã„ã¦", expanded=True):
    st.markdown("""
- **ç›®çš„**ï¼šåŸææ–™ï¼ˆéŠ…ãƒ»ã‚¢ãƒ«ãƒŸï¼‰ï¼‹ï¼ˆä»Šå¾Œï¼šè¼¸é€è²»ãƒ»è³ƒé‡‘ï¼‰ã®åŸä¾¡æ„Ÿã‚’â€œæœˆæ¬¡ã§â€ã¤ã‹ã‚€  
- **è¨ˆç®—**ï¼šUSD/ton Ã— USDJPY Ã· 1000 = **å††/kg**  
- **è¦‹æ–¹**ï¼šã‚°ãƒ©ãƒ•ã¯ **æœ€æ–°æœˆã‚’å¼·èª¿è¡¨ç¤º**ã€KPIã¯ **å‰æœˆæ¯”** ã¤ã  
""")

# ----------------------------
# Secrets / API key
# ----------------------------
FRED_API_KEY = st.secrets.get("FRED_API_KEY", "")
if not FRED_API_KEY:
    st.error("FRED_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼ˆStreamlit Secretsã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼‰")
    st.stop()

# e-Statï¼ˆã‚ãªãŸã®Secretsã¯ ESTAT_APP_ID ã«çµ±ä¸€ï¼‰
ESTAT_APP_ID = st.secrets.get("ESTAT_APP_ID", "")
if not ESTAT_APP_ID:
    st.warning("ESTAT_APP_ID ãŒæœªè¨­å®šã§ã™ï¼ˆè³ƒé‡‘ã®å–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ï¼‰")

# ----------------------------
# FRED fetch
# ----------------------------
@st.cache_data(ttl=60 * 60)
def fetch_fred(series_id: str, start: str = "2018-01-01") -> pd.Series:
    url = "https://api.stlouisfed.org/fred/series/observations"
    params = {
        "series_id": series_id,
        "api_key": FRED_API_KEY,
        "file_type": "json",
        "observation_start": start,
        "observation_end": date.today().isoformat(),
    }
    r = requests.get(url, params=params, timeout=30)
    r.raise_for_status()
    obs = r.json()["observations"]

    df = pd.DataFrame(obs)[["date", "value"]]
    df["date"] = pd.to_datetime(df["date"])
    df["value"] = pd.to_numeric(df["value"], errors="coerce")
    return df.dropna().set_index("date")["value"].sort_index()

# ----------------------------
# e-Stat fetch (raw JSON) + picker
# ----------------------------
@st.cache_data(ttl=60 * 60)
def fetch_estat_statsdata(stats_data_id: str, limit: int = 100000) -> dict:
    url = "https://api.e-stat.go.jp/rest/3.0/app/json/getStatsData"
    params = {
        "appId": ESTAT_APP_ID,
        "statsDataId": stats_data_id,
        "metaGetFlg": "Y",
        "cntGetFlg": "N",
        "limit": limit,
        "lang": "J",
    }
    r = requests.get(url, params=params, timeout=60)
    r.raise_for_status()
    return r.json()

def estat_pick_series(json_data: dict, industry_label_contains="è£½é€ æ¥­", item_label_contains=None) -> pd.Series:
    """
    è¿”ã£ã¦ããŸJSONã‹ã‚‰ã€ãƒ©ãƒ™ãƒ«æ¡ä»¶ï¼ˆä¾‹ï¼šè£½é€ æ¥­ï¼‰ã‚’å«ã‚€ã‚³ãƒ¼ãƒ‰ã‚’æ¨å®šã—ã¦
    ã–ã£ãã‚Šæ™‚ç³»åˆ—Seriesã‚’ä½œã‚‹ï¼ˆã¾ãšã¯â€œå‹•ãâ€å„ªå…ˆã®æŠ½å‡ºï¼‰ã€‚
    """
    gsd = json_data.get("GET_STATS_DATA", {})
    if "STATISTICAL_DATA" not in gsd:
        # å¤±æ•—æ™‚ã¯ç©ºSeries
        return pd.Series(dtype="float64")

    root = gsd["STATISTICAL_DATA"]
    class_inf = root["CLASS_INF"]["CLASS_OBJ"]
    values = root["DATA_INF"]["VALUE"]

    # CLASS_OBJ ã‚’ nameâ†’{code:label} ã«æ•´å½¢
    def to_map(obj):
        cls = obj["CLASS"]
        if isinstance(cls, dict):
            cls = [cls]
        return {c["@code"]: c["@name"] for c in cls}

    class_maps = {obj["@id"]: to_map(obj) for obj in class_inf}

    # ãƒ©ãƒ™ãƒ«ã‚’å«ã‚€ã‚³ãƒ¼ãƒ‰å€™è£œ
    industry_codes = set()
    item_codes = set()

    for _, cmap in class_maps.items():
        for code, name in cmap.items():
            if industry_label_contains and industry_label_contains in name:
                industry_codes.add(code)
            if item_label_contains and item_label_contains in name:
                item_codes.add(code)

    rows = []
    for v in values:
        t = v.get("@time") or v.get("@TIME")
        val = v.get("$")
        if t is None or val is None:
            continue

        dim_codes = [v[k] for k in v.keys() if k.startswith("@cat")]

        ok_industry = True if not industry_codes else any(c in industry_codes for c in dim_codes)
        ok_item = True if not item_label_contains else (True if not item_codes else any(c in item_codes for c in dim_codes))

        if ok_industry and ok_item:
            try:
                rows.append((t, float(val)))
            except ValueError:
                pass

    if not rows:
        return pd.Series(dtype="float64")

    s = pd.Series(
        data=[x[1] for x in rows],
        index=pd.to_datetime([x[0] for x in rows], errors="coerce")
    ).dropna().sort_index()

    # åŒæœˆé‡è¤‡ã¯æœ€å¾Œã‚’æ¡ç”¨
    s = s[~s.index.duplicated(keep="last")]
    return s

# ----------------------------
# WebKIT (PDF) fetch
# ----------------------------
@st.cache_data(ttl=60 * 60)
def fetch_webkit_index_from_pdf(pdf_url: str) -> pd.Series:
    import pdfplumber  # requirements.txt ã« pdfplumber ã‚’è¿½åŠ ã—ã¦ã­

    r = requests.get(pdf_url, timeout=60)
    r.raise_for_status()

    with pdfplumber.open(io.BytesIO(r.content)) as pdf:
        text = "\n".join(page.extract_text() or "" for page in pdf.pages)

    # å¹´åº¦è¡Œã£ã½ã„è¡Œã‚’æ‹¾ã†
    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
    rows = [ln for ln in lines if ("ä»¤å’Œ" in ln and "å¹´åº¦" in ln) or ("å¹³æˆ" in ln and "å¹´åº¦" in ln)]

    data = []
    for ln in rows:
        # ä¾‹ï¼š "ä»¤å’Œï¼—å¹´åº¦ 137 135 131 ..." ã¿ãŸã„ãªä¸¦ã³ã‚’æƒ³å®š
        nums = re.findall(r"\b\d+\b", ln)
        if len(nums) < 3:
            continue

        m = re.search(r"(ä»¤å’Œ|å¹³æˆ)\s*([0-9]+)\s*å¹´åº¦", ln)
        if not m:
            # ã‚¹ãƒšãƒ¼ã‚¹ç„¡ã—ã® "ä»¤å’Œï¼—å¹´åº¦" ã‚‚æ‹¾ã†
            m = re.search(r"(ä»¤å’Œ|å¹³æˆ)([0-9]+)å¹´åº¦", ln)
        if not m:
            continue

        era = m.group(1)
        n = int(m.group(2))
        values = list(map(int, nums))[:12]  # 4æœˆã€œ3æœˆã®æœ€å¤§12å€‹ã‚’æƒ³å®š
        data.append((era, n, values))

    series = []
    for era, n, vals in data:
        if era == "ä»¤å’Œ":
            start_year = 2018 + n  # ä»¤å’Œ1=2019 â†’ 2018+1
        else:
            start_year = 1988 + n  # å¹³æˆ1=1989 â†’ 1988+1

        months = list(range(4, 13)) + [1, 2, 3]
        years = [start_year] * 9 + [start_year + 1] * 3

        for y, mo, v in zip(years, months, vals):
            series.append((pd.Timestamp(y, mo, 1), v))

    if not series:
        return pd.Series(dtype="float64")

    s = pd.Series({d: v for d, v in series}).sort_index()
    s = s[~s.index.duplicated(keep="last")]
    return s

# ----------------------------
# Build base df (Copper/Aluminum)
# ----------------------------
copper = fetch_fred("PCOPPUSDM")
aluminum = fetch_fred("PALUMUSDM")
usdjpy = fetch_fred("EXJPUS")

df = pd.concat([copper, aluminum, usdjpy], axis=1, join="inner")
df.columns = ["copper_usd_ton", "aluminum_usd_ton", "usdjpy"]

df["copper_jpy_kg"] = df["copper_usd_ton"] * df["usdjpy"] / 1000
df["aluminum_jpy_kg"] = df["aluminum_usd_ton"] * df["usdjpy"] / 1000

# ----------------------------
# Latest / KPI
# ----------------------------
latest_date = df.index[-1]
latest_month_str = latest_date.strftime("%Y-%m")
latest_copper = float(df.loc[latest_date, "copper_jpy_kg"])
latest_aluminum = float(df.loc[latest_date, "aluminum_jpy_kg"])

delta_copper = None
delta_aluminum = None
if len(df) >= 2:
    prev_date = df.index[-2]
    delta_copper = latest_copper - float(df.loc[prev_date, "copper_jpy_kg"])
    delta_aluminum = latest_aluminum - float(df.loc[prev_date, "aluminum_jpy_kg"])

st.subheader("ğŸ“Œ æœ€æ–°æœˆã®åŸä¾¡ï¼ˆå††/kgï¼‰")
k1, k2, k3 = st.columns([1, 1, 1])
with k1:
    st.metric(
        label=f"éŠ…ï¼ˆ{latest_month_str}ï¼‰",
        value=f"{latest_copper:,.0f} å††/kg",
        delta=f"{delta_copper:+,.0f} å††/kg" if delta_copper is not None else None,
    )
with k2:
    st.metric(
        label=f"ã‚¢ãƒ«ãƒŸï¼ˆ{latest_month_str}ï¼‰",
        value=f"{latest_aluminum:,.0f} å††/kg",
        delta=f"{delta_aluminum:+,.0f} å††/kg" if delta_aluminum is not None else None,
    )
with k3:
    st.metric(
        label="æ›´æ–°æ—¥æ™‚",
        value=datetime.now().strftime("%Y-%m-%d %H:%M"),
    )

st.divider()

# ----------------------------
# Charts
# ----------------------------
def plot_with_latest_highlight(series: pd.Series, title: str, y_label: str):
    fig, ax = plt.subplots()
    ax.plot(series.index, series.values)

    ax.scatter(series.index[-1], series.values[-1], s=80, zorder=3)
    ax.annotate(
        f"{series.values[-1]:,.0f}",
        (series.index[-1], series.values[-1]),
        textcoords="offset points",
        xytext=(8, 8),
        ha="left",
    )

    ax.set_title(title)
    ax.set_xlabel("Month")
    ax.set_ylabel(y_label)
    ax.grid(True, alpha=0.3)
    st.pyplot(fig, clear_figure=True)

tab1, tab2, tab3, tab4, tab5 = st.tabs(
    ["ğŸŸ  éŠ…", "âšª ã‚¢ãƒ«ãƒŸ", "ğŸ“‰ ã¾ã¨ã‚", "ğŸ’´ è³ƒé‡‘ï¼ˆè£½é€ æ¥­ï¼‰", "ğŸšš ãƒˆãƒ©ãƒƒã‚¯é‹è³ƒæŒ‡æ•°"]
)

with tab1:
    st.subheader("éŠ… ä¾¡æ ¼æ¨ç§»ï¼ˆå††/kgï¼‰")
    plot_with_latest_highlight(df["copper_jpy_kg"], "Copper (JPY/kg)", "JPY/kg")
    st.caption("ãƒ‡ãƒ¼ã‚¿ï¼šFRED PCOPPUSDMï¼ˆUSD/tonï¼‰ã¨ EXJPUSï¼ˆUSDJPYï¼‰ã‹ã‚‰æ›ç®—")

with tab2:
    st.subheader("ã‚¢ãƒ«ãƒŸ ä¾¡æ ¼æ¨ç§»ï¼ˆå††/kgï¼‰")
    plot_with_latest_highlight(df["aluminum_jpy_kg"], "Aluminum (JPY/kg)", "JPY/kg")
    st.caption("ãƒ‡ãƒ¼ã‚¿ï¼šFRED PALUMUSDMï¼ˆUSD/tonï¼‰ã¨ EXJPUSï¼ˆUSDJPYï¼‰ã‹ã‚‰æ›ç®—")

with tab3:
    st.subheader("éŠ…ãƒ»ã‚¢ãƒ«ãƒŸï¼ˆå††/kgï¼‰åŒä¸€ã‚°ãƒ©ãƒ•")
    st.line_chart(df[["copper_jpy_kg", "aluminum_jpy_kg"]])
    st.caption("ã–ã£ãã‚Šæ¯”è¼ƒã—ãŸã„äººå‘ã‘ï¼ˆè©³ç´°ã¯å„ã‚¿ãƒ–ã¸ï¼‰")

with tab4:
    st.subheader("è£½é€ æ¥­ã®è³ƒé‡‘ï¼ˆe-Statï¼‰")
    if not ESTAT_APP_ID:
        st.info("ESTAT_APP_ID ãŒæœªè¨­å®šã®ãŸã‚ã€è³ƒé‡‘ãƒ‡ãƒ¼ã‚¿ã¯è¡¨ç¤ºã—ã¾ã›ã‚“ã€‚")
    else:
        # ã‚ãªãŸã®e-Stat URLã«ã‚ã£ãŸ stat_infid ã‚’ statsDataId ã¨ã—ã¦è©¦ã™
        stats_data_id = "000040277086"
        estat_json = fetch_estat_statsdata(stats_data_id)

        # å¤±æ•—æ™‚ã«ç†ç”±ã‚’è¡¨ç¤ºï¼ˆèµ¤å¡—ã‚Šå¯¾ç­–ï¼‰
        gsd = estat_json.get("GET_STATS_DATA", {})
        if "STATISTICAL_DATA" not in gsd:
            st.error("e-Stat APIãŒ STATISTICAL_DATA ã‚’è¿”ã—ã¦ã„ã¾ã›ã‚“ï¼ˆå–å¾—å¤±æ•—ï¼‰")
            st.write("RESULT:", gsd.get("RESULT"))
            st.write("ERROR_MSG:", gsd.get("ERROR_MSG"))
            st.stop()

        # ã¾ãšã¯ â€œå‹•ãå„ªå…ˆâ€ ã®æŠ½å‡ºï¼ˆè£½é€ æ¥­ã‚’å«ã‚€ç³»åˆ—ã‚’æ‹¾ã†ï¼‰
        wage_mfg = estat_pick_series(estat_json, industry_label_contains="è£½é€ æ¥­", item_label_contains=None)

        if wage_mfg.empty:
            st.warning("è£½é€ æ¥­ã®ç³»åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆçµ±è¨ˆè¡¨ID/æŠ½å‡ºæ¡ä»¶ã®èª¿æ•´ãŒå¿…è¦ã§ã™ï¼‰")
        else:
            st.line_chart(wage_mfg.rename("wage_mfg"))
            st.caption("â€»ã¾ãšã¯è£½é€ æ¥­ã‚’å«ã‚€ç³»åˆ—ã‚’æ¨å®šæŠ½å‡ºã—ã¦ã„ã¾ã™ã€‚æ¬¡ã«ã€ç¾é‡‘çµ¦ä¸ç·é¡ï¼ˆå††ï¼‰ã€ã«çµã‚‹èª¿æ•´ãŒå¯èƒ½ã§ã™ã€‚")

with tab5:
    st.subheader("å›½å†…ãƒˆãƒ©ãƒƒã‚¯é‹è³ƒæŒ‡æ•°ï¼ˆWebKIT æˆç´„é‹è³ƒæŒ‡æ•°ï¼‰")
    pdf_url = "https://jta.or.jp/pdf/kit_release/202512.pdf"  # ã¾ãšã¯å›ºå®šã§å‹•ä½œç¢ºèª
    try:
        webkit = fetch_webkit_index_from_pdf(pdf_url)
        if webkit.empty:
            st.warning("PDFã‹ã‚‰æŒ‡æ•°ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸï¼ˆPDFå½¢å¼ãŒå¤‰ã‚ã£ã¦ã„ã‚‹å¯èƒ½æ€§ï¼‰")
        else:
            st.line_chart(webkit.rename("webkit_freight_index"))
            st.caption(f"å‡ºå…¸PDFï¼š{pdf_url}")
    except Exception as e:
        st.error("PDFã®å–å¾—/è§£æã§ã‚¨ãƒ©ãƒ¼ã«ãªã‚Šã¾ã—ãŸã€‚requirements.txt ã« pdfplumber ãŒå…¥ã£ã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        st.write(str(e))

st.divider()

# ----------------------------
# Data table + download (Copper/Aluminum)
# ----------------------------
st.subheader("ğŸ“„ ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼šéŠ…ãƒ»ã‚¢ãƒ«ãƒŸï¼‰")

download_df = df[["copper_jpy_kg", "aluminum_jpy_kg"]].copy()
download_df = download_df.rename(
    columns={
        "copper_jpy_kg": "copper_jpy_per_kg",
        "aluminum_jpy_kg": "aluminum_jpy_per_kg",
    }
)
download_df.index.name = "date"

with st.expander("è¡¨ã‚’è¡¨ç¤º"):
    st.dataframe(download_df.tail(24), use_container_width=True)

csv_bytes = download_df.to_csv(encoding="utf-8-sig").encode("utf-8-sig")
st.download_button(
    label="ğŸ“¥ CSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
    data=csv_bytes,
    file_name="copper_aluminum_jpy_per_kg_monthly.csv",
    mime="text/csv",
)

