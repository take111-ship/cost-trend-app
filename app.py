import re
import io
import requests
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt
from datetime import date, datetime
import pdfplumber

# ----------------------------
# Page / UI
# ----------------------------
st.set_page_config(
    page_title="åŸä¾¡ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ï¼ˆéŠ…ãƒ»ã‚¢ãƒ«ãƒŸãƒ»é‹è³ƒãƒ»è³ƒé‡‘ï¼‰",
    page_icon="ğŸ“ˆ",
    layout="wide",
)

st.title("ğŸ“ˆ åŸä¾¡ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ï¼ˆéŠ…ãƒ»ã‚¢ãƒ«ãƒŸãƒ»é‹è³ƒãƒ»è³ƒé‡‘ï¼‰")
st.caption("éŠ…/ã‚¢ãƒ«ãƒŸã¯FREDï¼ˆUSD/tonï¼‰Ã—USDJPYã§å††/kgæ›ç®—ã€‚é‹è³ƒã¯WebKIT PDFã€‚è³ƒé‡‘ã¯e-Stat APIã€‚")

with st.sidebar:
    st.header("ğŸ“š ãƒ‡ãƒ¼ã‚¿å…ƒãƒªãƒ³ã‚¯ï¼ˆå›ºå®šè¡¨ç¤ºï¼‰")
    st.markdown("""
**â–  FRED**
- éŠ…ï¼ˆUSD/tonï¼‰: https://fred.stlouisfed.org/series/PCOPPUSDM  
- ã‚¢ãƒ«ãƒŸï¼ˆUSD/tonï¼‰: https://fred.stlouisfed.org/series/PALUMUSDM  
- ç‚ºæ›¿ï¼ˆUSD/JPYï¼‰: https://fred.stlouisfed.org/series/EXJPUS  

**â–  WebKITï¼ˆå…¨ãƒˆå”ï¼‰**
- å…¬è¡¨ãƒšãƒ¼ã‚¸: https://jta.or.jp/member/keiei/kit_release.html  

**â–  e-Stat API**
- ä»•æ§˜/æ¡ˆå†…: https://www.e-stat.go.jp/api/api-info/e-stat-manual  
""")

with st.expander("ã“ã®ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã«ã¤ã„ã¦", expanded=True):
    st.markdown("""
- **éŠ…ãƒ»ã‚¢ãƒ«ãƒŸï¼ˆå††/kgï¼‰**ï¼šFREDã®æœˆæ¬¡ï¼ˆUSD/tonï¼‰Ã— USDJPY Ã· 1000  
- **é‹è³ƒæŒ‡æ•°**ï¼šWebKITæˆç´„é‹è³ƒæŒ‡æ•°ï¼ˆPDFå†…ã®æœˆåˆ¥è¡¨ï¼‰  
- **è³ƒé‡‘ï¼ˆè£½é€ æ¥­ï¼‰**ï¼še-Stat APIã‹ã‚‰å–å¾—ï¼ˆçµ±è¨ˆè¡¨ID=statsDataId ã‚’æ¤œç´¢ã—ã¦åˆ©ç”¨ï¼‰  
""")

# ----------------------------
# Secrets
# ----------------------------
FRED_API_KEY = st.secrets.get("FRED_API_KEY", "")
if not FRED_API_KEY:
    st.error("FRED_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼ˆStreamlit Secrets ã‚’ç¢ºèªï¼‰")
    st.stop()

ESTAT_APP_ID = st.secrets.get("ESTAT_APP_ID", "")
if not ESTAT_APP_ID:
    st.warning("ESTAT_APP_ID ãŒæœªè¨­å®šã§ã™ï¼ˆè³ƒé‡‘ãƒ‘ãƒ¼ãƒˆã¯å‹•ãã¾ã›ã‚“ï¼‰ã€‚Streamlit Secrets ã«è¿½åŠ ã—ã¦ãã ã•ã„ã€‚")

# ----------------------------
# FRED fetch
# ----------------------------
@st.cache_data(ttl=60 * 60)
def fetch_fred(series_id: str, start: str = "2018-01-01") -> pd.Series:
    url = "https://api.stlouisfed.org/fred/series/observations"
    params = {
        "series_id": series_id,
        "api_key": FRED_API_KEY,
        "file_type": "json",
        "observation_start": start,
        "observation_end": date.today().isoformat(),
    }
    r = requests.get(url, params=params, timeout=30)
    r.raise_for_status()
    obs = r.json()["observations"]

    df = pd.DataFrame(obs)[["date", "value"]]
    df["date"] = pd.to_datetime(df["date"])
    df["value"] = pd.to_numeric(df["value"], errors="coerce")
    s = df.dropna().set_index("date")["value"].sort_index()
    return s

# ----------------------------
# WebKIT (latest pdf URL from JTA page)
# ----------------------------
@st.cache_data(ttl=60 * 60)
def webkit_latest_pdf_url() -> str:
    # JTAã®ãƒšãƒ¼ã‚¸ã«æ¯æœˆã®PDFãƒªãƒ³ã‚¯ãŒè¼‰ã‚‹ï¼ˆä¾‹: /pdf/kit_release/202512.pdfï¼‰
    url = "https://jta.or.jp/member/keiei/kit_release.html"
    r = requests.get(url, timeout=30)
    r.raise_for_status()
    html = r.text

    links = re.findall(r"/pdf/kit_release/(\d{6})\.pdf", html)
    if not links:
        raise ValueError("WebKITã®PDFãƒªãƒ³ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ãƒšãƒ¼ã‚¸æ§‹é€ ãŒå¤‰ã‚ã£ãŸå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")

    latest_yyyymm = max(links)  # æ–‡å­—åˆ—æ¯”è¼ƒã§OKï¼ˆYYYYMMï¼‰
    return f"https://jta.or.jp/pdf/kit_release/{latest_yyyymm}.pdf"

@st.cache_data(ttl=60 * 60)
def fetch_webkit_index_from_pdf(pdf_url: str) -> pd.Series:
    r = requests.get(pdf_url, timeout=60)
    r.raise_for_status()

    with pdfplumber.open(io.BytesIO(r.content)) as pdf:
        text = "\n".join((page.extract_text() or "") for page in pdf.pages)

    # ã€Œæˆç´„é‹è³ƒæŒ‡æ•°ï¼ˆæœˆåˆ¥ï¼‰ã®æ¨ç§»ã€ã®è¡¨ã¯
    # å¹³æˆï¼’ï¼’å¹´åº¦ 100 98 ... ã®ã‚ˆã†ãªè¡Œã§å‡ºã¦ãã‚‹ï¼ˆPDFã®ãƒšãƒ¼ã‚¸2ä»˜è¿‘ï¼‰ :contentReference[oaicite:2]{index=2}
    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]

    # ã€Œå¹³æˆã€ã€Œä»¤å’Œã€ã©ã¡ã‚‰ã‚‚æ‹¾ã†ï¼ˆå…¨è§’æ•°å­—ã§ã‚‚OKã«ã™ã‚‹ãŸã‚ \d ã§ã¯ãªãæ•°å­—æŠ½å‡ºæ–¹å¼ï¼‰
    target_rows = []
    for ln in lines:
        if re.match(r"^(å¹³æˆ|ä»¤å’Œ).+å¹´åº¦", ln):
            target_rows.append(ln)

    if not target_rows:
        return pd.Series(dtype="float64")

    data_points = []
    for ln in target_rows:
        # è¡Œã‹ã‚‰æ•°å€¤ã‚’å…¨éƒ¨æ‹¾ã†ï¼ˆå¹´åº¦åã®ä¸­ã®æ•°å­—ã‚‚æ‹¾ã†ã®ã§ã€12å€‹ã ã‘ä½¿ã†ï¼‰
        nums = re.findall(r"\d+", ln)
        if len(nums) < 12:
            # 12å€‹ãªã„è¡Œã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆä»¤å’Œï¼—å¹´åº¦ã¿ãŸã„ã«é€”ä¸­ã¾ã§ã®å¯èƒ½æ€§ã¯ã‚ã‚‹ã®ã§å¾Œã§è¨±å®¹ï¼‰
            pass

        # å¹´åº¦ãƒ©ãƒ™ãƒ«æŠ½å‡º
        m = re.search(r"(å¹³æˆ|ä»¤å’Œ)\s*([0-9]+)\s*å¹´åº¦", ln)
        if not m:
            m = re.search(r"(å¹³æˆ|ä»¤å’Œ)([0-9]+)å¹´åº¦", ln)
        if not m:
            continue

        era = m.group(1)
        n = int(m.group(2))

        # å¹´åº¦é–‹å§‹ã®è¥¿æš¦ï¼ˆ4æœˆé–‹å§‹ï¼‰
        if era == "ä»¤å’Œ":
            start_year = 2018 + n   # ä»¤å’Œ1=2019
        else:
            start_year = 1988 + n   # å¹³æˆ1=1989

        # ã“ã®è¡Œã®ã€Œæœˆåˆ¥æŒ‡æ•°ã€éƒ¨åˆ†ã¯12å€‹ï¼ˆ4æœˆã€œ3æœˆï¼‰
        # ãŸã ã— nums ã«ã¯å¹´åº¦ç•ªå·ãªã©ãŒæ··ã–ã‚‹ã®ã§ã€æœ€å¾Œã®æ–¹ã«ã‚ã‚‹æ•°å€¤ç¾¤ã‚’å„ªå…ˆ
        # â†’ è¡Œæœ«å´ã‹ã‚‰æœ€å¤§12å€‹å–ã‚‹
        month_vals = list(map(int, nums[-12:]))

        months = list(range(4, 13)) + [1, 2, 3]
        years = [start_year]*9 + [start_year+1]*3

        for y, mo, v in zip(years, months, month_vals):
            data_points.append((pd.Timestamp(y, mo, 1), v))

    if not data_points:
        return pd.Series(dtype="float64")

    s = pd.Series({d: v for d, v in data_points}).sort_index()
    s = s[~s.index.duplicated(keep="last")]
    return s

# ----------------------------
# e-Stat: getStatsList -> pick statsDataId -> getStatsData
# ----------------------------
@st.cache_data(ttl=60 * 60)
def estat_get_stats_list(search_word: str, stats_code: str = "00450071", limit: int = 100) -> list[dict]:
    # ä»•æ§˜: çµ±è¨ˆè¡¨æƒ…å ±å–å¾—ï¼ˆgetStatsListï¼‰ :contentReference[oaicite:3]{index=3}
    url = "https://api.e-stat.go.jp/rest/3.0/app/json/getStatsList"
    params = {
        "appId": ESTAT_APP_ID,
        "searchWord": search_word,
        "statsCode": stats_code,
        "limit": limit,
        "lang": "J",
    }
    r = requests.get(url, params=params, timeout=60)
    r.raise_for_status()
    data = r.json()

    result = data.get("GET_STATS_LIST", {}).get("DATALIST_INF", {})
    tables = result.get("TABLE_INF", [])
    if isinstance(tables, dict):
        tables = [tables]

    out = []
    for t in tables:
        out.append({
            "statsDataId": t.get("@id"),
            "title": t.get("TITLE", ""),
            "updated": t.get("UPDATED_DATE", ""),
        })
    return [x for x in out if x["statsDataId"]]

@st.cache_data(ttl=60 * 60)
def estat_get_stats_data(stats_data_id: str, limit: int = 100000) -> dict:
    url = "https://api.e-stat.go.jp/rest/3.0/app/json/getStatsData"
    params = {
        "appId": ESTAT_APP_ID,
        "statsDataId": stats_data_id,
        "metaGetFlg": "Y",
        "cntGetFlg": "N",
        "limit": limit,
        "lang": "J",
    }
    r = requests.get(url, params=params, timeout=60)
    r.raise_for_status()
    return r.json()

def estat_build_class_maps(root: dict) -> dict:
    class_obj = root["CLASS_INF"]["CLASS_OBJ"]

    def to_map(obj):
        cls = obj["CLASS"]
        if isinstance(cls, dict):
            cls = [cls]
        return {c["@code"]: c["@name"] for c in cls}

    return {obj["@id"]: to_map(obj) for obj in class_obj}

def estat_series_from_statsdata(json_data: dict, *, industry_contains: str, item_contains: str) -> pd.Series:
    gsd = json_data.get("GET_STATS_DATA", {})
    stat = gsd.get("STATISTICAL_DATA")
    if not stat:
        return pd.Series(dtype="float64")

    class_maps = estat_build_class_maps(stat)
    values = stat["DATA_INF"]["VALUE"]
    if isinstance(values, dict):
        values = [values]

    # ã€Œã©ã®catãŒä½•ã‹ã€ã¯çµ±è¨ˆè¡¨ã”ã¨ã«å¤‰ã‚ã‚‹ã®ã§ã€åå‰ã§ã‚†ã‚‹ãæ¢ã™
    # â†’ class_maps ã®æ—¥æœ¬èªãƒ©ãƒ™ãƒ«ã« "è£½é€ æ¥­" ã‚„ "ç¾é‡‘çµ¦ä¸" ã‚’å«ã‚€ã‚³ãƒ¼ãƒ‰ã‚’é›†ã‚ã‚‹
    industry_codes = set()
    item_codes = set()

    for dim_id, cmap in class_maps.items():
        for code, name in cmap.items():
            if industry_contains in name:
                industry_codes.add(code)
            if item_contains in name:
                item_codes.add(code)

    rows = []
    for v in values:
        # time ã¯ "@time" ãŒåŸºæœ¬
        t = v.get("@time")
        val = v.get("$")
        if t is None or val is None:
            continue

        dim_codes = [v[k] for k in v.keys() if k.startswith("@cat")]
        if industry_codes and (not any(c in industry_codes for c in dim_codes)):
            continue
        if item_codes and (not any(c in item_codes for c in dim_codes)):
            continue

        try:
            rows.append((t, float(val)))
        except:
            continue

    if not rows:
        return pd.Series(dtype="float64")

    # time ã¯ "202501" ã‚„ "2025-01" ç­‰ãŒæ··ã–ã‚‹ã“ã¨ãŒã‚ã‚‹ã®ã§ä¸¡å¯¾å¿œ
    idx = []
    vals = []
    for t, v in rows:
        t_str = str(t)
        dt = None
        for fmt in ("%Y%m", "%Y-%m", "%Y-%m-%d"):
            try:
                dt = datetime.strptime(t_str, fmt)
                break
            except:
                pass
        if dt is None:
            # æœ€å¾Œã®æ‰‹æ®µ
            try:
                dt = pd.to_datetime(t_str)
            except:
                continue
        idx.append(pd.Timestamp(dt.year, dt.month, 1))
        vals.append(v)

    s = pd.Series(vals, index=idx).sort_index()
    s = s[~s.index.duplicated(keep="last")]
    return s

# ----------------------------
# Common plot helper
# ----------------------------
def plot_with_latest_highlight(series: pd.Series, title: str, y_label: str):
    if series.empty:
        st.info("ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚")
        return

    fig, ax = plt.subplots()
    ax.plot(series.index, series.values)
    ax.scatter(series.index[-1], series.values[-1], s=80, zorder=3)
    ax.annotate(
        f"{series.values[-1]:,.0f}",
        (series.index[-1], series.values[-1]),
        textcoords="offset points",
        xytext=(8, 8),
        ha="left",
    )
    ax.set_title(title)
    ax.set_xlabel("Month")
    ax.set_ylabel(y_label)
    ax.grid(True, alpha=0.3)
    st.pyplot(fig, clear_figure=True)

# ----------------------------
# 1) Copper / Aluminum (FRED)
# ----------------------------
copper = fetch_fred("PCOPPUSDM")
aluminum = fetch_fred("PALUMUSDM")
usdjpy = fetch_fred("EXJPUS")

df = pd.concat([copper, aluminum, usdjpy], axis=1, join="inner")
df.columns = ["copper_usd_ton", "aluminum_usd_ton", "usdjpy"]
df["copper_jpy_kg"] = df["copper_usd_ton"] * df["usdjpy"] / 1000
df["aluminum_jpy_kg"] = df["aluminum_usd_ton"] * df["usdjpy"] / 1000

latest_date = df.index[-1]
latest_month_str = latest_date.strftime("%Y-%m")

latest_copper = float(df.loc[latest_date, "copper_jpy_kg"])
latest_aluminum = float(df.loc[latest_date, "aluminum_jpy_kg"])

delta_copper = None
delta_aluminum = None
if len(df) >= 2:
    prev_date = df.index[-2]
    delta_copper = latest_copper - float(df.loc[prev_date, "copper_jpy_kg"])
    delta_aluminum = latest_aluminum - float(df.loc[prev_date, "aluminum_jpy_kg"])

st.subheader("ğŸ“Œ æœ€æ–°æœˆã®åŸä¾¡ï¼ˆå††/kgï¼‰")
k1, k2, k3 = st.columns([1, 1, 1])
with k1:
    st.metric(
        label=f"éŠ…ï¼ˆ{latest_month_str}ï¼‰",
        value=f"{latest_copper:,.0f} å††/kg",
        delta=f"{delta_copper:+,.0f} å††/kg" if delta_copper is not None else None,
    )
with k2:
    st.metric(
        label=f"ã‚¢ãƒ«ãƒŸï¼ˆ{latest_month_str}ï¼‰",
        value=f"{latest_aluminum:,.0f} å††/kg",
        delta=f"{delta_aluminum:+,.0f} å††/kg" if delta_aluminum is not None else None,
    )
with k3:
    st.metric(label="æ›´æ–°æ—¥æ™‚", value=datetime.now().strftime("%Y-%m-%d %H:%M"))

tab1, tab2, tab3 = st.tabs(["ğŸŸ  éŠ…", "âšª ã‚¢ãƒ«ãƒŸ", "ğŸ“‰ ã¾ã¨ã‚ï¼ˆåŒä¸€ã‚°ãƒ©ãƒ•ï¼‰"])
with tab1:
    st.subheader("éŠ… ä¾¡æ ¼æ¨ç§»ï¼ˆå††/kgï¼‰")
    plot_with_latest_highlight(df["copper_jpy_kg"], "Copper (JPY/kg)", "JPY/kg")
with tab2:
    st.subheader("ã‚¢ãƒ«ãƒŸ ä¾¡æ ¼æ¨ç§»ï¼ˆå††/kgï¼‰")
    plot_with_latest_highlight(df["aluminum_jpy_kg"], "Aluminum (JPY/kg)", "JPY/kg")
with tab3:
    st.subheader("éŠ…ãƒ»ã‚¢ãƒ«ãƒŸï¼ˆå††/kgï¼‰åŒä¸€ã‚°ãƒ©ãƒ•")
    st.line_chart(df[["copper_jpy_kg", "aluminum_jpy_kg"]])

st.divider()

# ----------------------------
# 2) WebKIT freight index
# ----------------------------
st.subheader("ğŸšš å›½å†…ãƒˆãƒ©ãƒƒã‚¯é‹è³ƒæŒ‡æ•°ï¼ˆWebKIT æˆç´„é‹è³ƒæŒ‡æ•°ï¼‰")

try:
    pdf_url = webkit_latest_pdf_url()
    st.caption(f"æœ€æ–°PDF: {pdf_url}")
    webkit = fetch_webkit_index_from_pdf(pdf_url)

    if webkit.empty:
        st.warning("PDFã‹ã‚‰æœˆåˆ¥æŒ‡æ•°ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸï¼ˆPDFæ§‹é€ ãŒå¤‰ã‚ã£ãŸå¯èƒ½æ€§ï¼‰ã€‚")
    else:
        plot_with_latest_highlight(webkit, "WebKIT Freight Index", "Index (2010-04=100)")
        st.line_chart(webkit.rename("webkit_freight_index"))
except Exception as e:
    st.error(f"WebKITå–å¾—ã§ã‚¨ãƒ©ãƒ¼: {e}")

st.divider()

# ----------------------------
# 3) e-Stat wage (manufacturing)
# ----------------------------
st.subheader("ğŸ’´ è£½é€ æ¥­ã®è³ƒé‡‘ï¼ˆe-Stat APIï¼‰")

if not ESTAT_APP_ID:
    st.info("ESTAT_APP_ID ãŒæœªè¨­å®šã®ãŸã‚ã€ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯åœæ­¢ã—ã¦ã„ã¾ã™ã€‚")
else:
    # ã¾ãšçµ±è¨ˆè¡¨ã‚’æ¤œç´¢ï¼ˆstatsDataIdã‚’è‡ªå‹•ã§å€™è£œã«ã™ã‚‹ï¼‰
    search_word = st.text_input(
        "çµ±è¨ˆè¡¨ã®æ¤œç´¢ãƒ¯ãƒ¼ãƒ‰ï¼ˆä¾‹ï¼‰",
        value="æ¯æœˆå‹¤åŠ´çµ±è¨ˆèª¿æŸ» å…¨å›½èª¿æŸ» ç¾é‡‘çµ¦ä¸ç·é¡",
        help="ã“ã“ã§å€™è£œã®çµ±è¨ˆè¡¨ï¼ˆstatsDataIdï¼‰ã‚’æ¤œç´¢ã—ã¾ã™ã€‚",
    )

    try:
        tables = estat_get_stats_list(search_word=search_word, stats_code="00450071", limit=100)

        if not tables:
            st.warning("å€™è£œãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚æ¤œç´¢ãƒ¯ãƒ¼ãƒ‰ã‚’å¤‰ãˆã¦ãã ã•ã„ã€‚")
        else:
            # ã‚¿ã‚¤ãƒˆãƒ«ã‚’è¦‹ãªãŒã‚‰é¸ã¹ã‚‹ã‚ˆã†ã«ã™ã‚‹ï¼ˆã“ã‚ŒãŒä¸€ç•ªç¢ºå®Ÿï¼‰
            options = {f'{t["statsDataId"]} | {t["title"]}': t["statsDataId"] for t in tables}
            selected_key = st.selectbox("å–å¾—ã™ã‚‹çµ±è¨ˆè¡¨ã‚’é¸æŠï¼ˆstatsDataIdï¼‰", list(options.keys()))
            stats_data_id = options[selected_key]

            estat_json = estat_get_stats_data(stats_data_id)

            # â€œè£½é€ æ¥­â€ Ã— â€œç¾é‡‘çµ¦ä¸ç·é¡â€ ã‚’ã‚†ã‚‹ãæŠ½å‡º
            wage_mfg = estat_series_from_statsdata(
                estat_json,
                industry_contains="è£½é€ æ¥­",
                item_contains="ç¾é‡‘çµ¦ä¸ç·é¡",
            )

            if wage_mfg.empty:
                st.warning("è£½é€ æ¥­Ã—ç¾é‡‘çµ¦ä¸ç·é¡ã®ç³»åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                st.info("ãƒ’ãƒ³ãƒˆï¼šitem_contains ã‚’ 'ãã¾ã£ã¦æ”¯çµ¦ã™ã‚‹çµ¦ä¸' ãªã©ã«å¤‰ãˆã‚‹ã¨è¦‹ã¤ã‹ã‚‹è¡¨ã‚‚ã‚ã‚Šã¾ã™ã€‚")
            else:
                plot_with_latest_highlight(wage_mfg, "Manufacturing Wage (e-Stat)", "Value")
                st.line_chart(wage_mfg.rename("wage_mfg"))

    except Exception as e:
        st.error(f"e-Statå–å¾—ã§ã‚¨ãƒ©ãƒ¼: {e}")

