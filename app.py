import io
import re
import requests
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt
from datetime import date, datetime

# ----------------------------
# Page / UI
# ----------------------------
st.set_page_config(
    page_title="éŠ…ãƒ»ã‚¢ãƒ«ãƒŸ åŸä¾¡ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰",
    page_icon="ğŸ“ˆ",
    layout="wide",
)

st.title("ğŸ“ˆ éŠ…ãƒ»ã‚¢ãƒ«ãƒŸ åŸä¾¡æŒ‡æ¨™ï¼ˆå††/kgãƒ»æœˆæ¬¡ï¼‰")
st.caption("FREDã®æœˆæ¬¡ãƒ‡ãƒ¼ã‚¿ï¼ˆUSD/tonï¼‰Ã— USDJPY ã‹ã‚‰å††/kgã«æ›ç®—ã—ã¦è¡¨ç¤ºã—ã¾ã™ã€‚")

with st.sidebar:
    st.header("ğŸ“š ãƒ‡ãƒ¼ã‚¿å…ƒ / è¨ˆç®—æ¡ä»¶")
    st.markdown("""
**â–  ãƒ‡ãƒ¼ã‚¿å…ƒ**
- FRED  
  - éŠ…ï¼ˆPCOPPUSDMï¼‰ https://fred.stlouisfed.org/series/PCOPPUSDM  
  - ã‚¢ãƒ«ãƒŸï¼ˆPALUMUSDMï¼‰ https://fred.stlouisfed.org/series/PALUMUSDM  
  - ç‚ºæ›¿ USD/JPYï¼ˆEXJPUSï¼‰ https://fred.stlouisfed.org/series/EXJPUS  
- e-Statï¼ˆæ¯æœˆå‹¤åŠ´çµ±è¨ˆèª¿æŸ» ãªã©ï¼‰ https://www.e-stat.go.jp/
- å…¨æ—¥æœ¬ãƒˆãƒ©ãƒƒã‚¯å”ä¼š WebKITï¼ˆæˆç´„é‹è³ƒæŒ‡æ•°ï¼‰ https://jta.or.jp/

---

**â–  è¨ˆç®—å¼ï¼ˆéŠ…ãƒ»ã‚¢ãƒ«ãƒŸï¼‰**  
USD/ton Ã— USD/JPY Ã· 1000 = å††/kg  

---

**â–  æ›´æ–°é »åº¦**  
æœˆæ¬¡ï¼ˆå„ãƒ‡ãƒ¼ã‚¿æä¾›å…ƒã®æ›´æ–°ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã«ä¾å­˜ï¼‰
""")

with st.expander("ã“ã®ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã«ã¤ã„ã¦", expanded=True):
    st.markdown("""
- **ç›®çš„**ï¼šåŸææ–™ï¼ˆéŠ…ãƒ»ã‚¢ãƒ«ãƒŸï¼‰ï¼‹ï¼ˆè¼¸é€è²»ãƒ»è³ƒé‡‘ï¼‰ã®åŸä¾¡æ„Ÿã‚’â€œæœˆæ¬¡ã§â€ã¤ã‹ã‚€  
- **è¦‹æ–¹**ï¼šã‚°ãƒ©ãƒ•ã¯ **æœ€æ–°æœˆã‚’å¼·èª¿è¡¨ç¤º**ã€KPIã¯ **å‰æœˆæ¯”** ã¤ã  
- **æ³¨æ„**ï¼šè¼¸é€è²»ãƒ»è³ƒé‡‘ã¯å…¬é–‹çµ±è¨ˆã®ä»•æ§˜å¤‰æ›´ã§å–å¾—ã§ããªã„å ´åˆãŒã‚ã‚Šã¾ã™ï¼ˆãã®å ´åˆã¯è­¦å‘Šè¡¨ç¤ºï¼‰
""")

# ----------------------------
# Secrets / API key
# ----------------------------
FRED_API_KEY = st.secrets.get("FRED_API_KEY", "")
if not FRED_API_KEY:
    st.error("FRED_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼ˆStreamlit Secretsã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼‰")
    st.stop()

# e-Statï¼ˆã‚­ãƒ¼åã¯ ESTAT_APP_ID ã«çµ±ä¸€ï¼‰
ESTAT_APP_ID = st.secrets.get("ESTAT_APP_ID", "")
if not ESTAT_APP_ID:
    st.warning("ESTAT_APP_ID ãŒæœªè¨­å®šã§ã™ï¼ˆè³ƒé‡‘ã®å–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ï¼‰")

# ----------------------------
# FRED fetch
# ----------------------------
@st.cache_data(ttl=60 * 60)
def fetch_fred(series_id: str, start: str = "2018-01-01") -> pd.Series:
    url = "https://api.stlouisfed.org/fred/series/observations"
    params = {
        "series_id": series_id,
        "api_key": FRED_API_KEY,
        "file_type": "json",
        "observation_start": start,
        "observation_end": date.today().isoformat(),
    }
    r = requests.get(url, params=params, timeout=30)
    r.raise_for_status()
    obs = r.json()["observations"]

    df = pd.DataFrame(obs)[["date", "value"]]
    df["date"] = pd.to_datetime(df["date"])
    df["value"] = pd.to_numeric(df["value"], errors="coerce")
    return df.dropna().set_index("date")["value"].sort_index()

# ----------------------------
# e-Stat fetch (raw JSON) + picker
# ----------------------------
@st.cache_data(ttl=60 * 60)
def fetch_estat_statsdata(stats_data_id: str, limit: int = 100000) -> dict:
    url = "https://api.e-stat.go.jp/rest/3.0/app/json/getStatsData"
    params = {
        "appId": ESTAT_APP_ID,
        "statsDataId": stats_data_id,
        "metaGetFlg": "Y",
        "cntGetFlg": "N",
        "limit": limit,
        "lang": "J",
    }
    r = requests.get(url, params=params, timeout=60)
    r.raise_for_status()
    return r.json()

def estat_pick_series(json_data: dict, industry_label_contains="è£½é€ æ¥­", item_label_contains=None) -> pd.Series:
    """
    è¿”ã£ã¦ããŸJSONã‹ã‚‰ãƒ©ãƒ™ãƒ«æ¡ä»¶ï¼ˆä¾‹ï¼šè£½é€ æ¥­ï¼‰ã‚’å«ã‚€ã‚³ãƒ¼ãƒ‰ã‚’æ¨å®šã—ã¦
    ã–ã£ãã‚Šæ™‚ç³»åˆ—Seriesã‚’ä½œã‚‹ï¼ˆã¾ãšã¯â€œå‹•ãâ€å„ªå…ˆã®æŠ½å‡ºï¼‰ã€‚
    """
    gsd = json_data.get("GET_STATS_DATA", {})
    if "STATISTICAL_DATA" not in gsd:
        return pd.Series(dtype="float64")

    root = gsd["STATISTICAL_DATA"]
    class_inf = root["CLASS_INF"]["CLASS_OBJ"]
    values = root["DATA_INF"]["VALUE"]

    def to_map(obj):
        cls = obj["CLASS"]
        if isinstance(cls, dict):
            cls = [cls]
        return {c["@code"]: c["@name"] for c in cls}

    class_maps = {obj["@id"]: to_map(obj) for obj in class_inf}

    industry_codes = set()
    item_codes = set()
    for _, cmap in class_maps.items():
        for code, name in cmap.items():
            if industry_label_contains and industry_label_contains in name:
                industry_codes.add(code)
            if item_label_contains and item_label_contains in name:
                item_codes.add(code)

    rows = []
    for v in values:
        t = v.get("@time") or v.get("@TIME")
        val = v.get("$")
        if t is None or val is None:
            continue

        dim_codes = [v[k] for k in v.keys() if k.startswith("@cat")]

        ok_industry = True if not industry_codes else any(c in industry_codes for c in dim_codes)
        ok_item = True if not item_label_contains else (True if not item_codes else any(c in item_codes for c in dim_codes))

        if ok_industry and ok_item:
            try:
                rows.append((t, float(val)))
            except ValueError:
                pass

    if not rows:
        return pd.Series(dtype="float64")

    s = pd.Series(
        data=[x[1] for x in rows],
        index=pd.to_datetime([x[0] for x in rows], errors="coerce")
    ).dropna().sort_index()
    s = s[~s.index.duplicated(keep="last")]
    return s

# ----------------------------
# WebKIT latest PDF finder + PDF parser
# ----------------------------
@st.cache_data(ttl=60 * 60)
def get_latest_webkit_pdf_url() -> str:
    """
    å…¨ãƒˆå”ã®WebKITãƒªãƒªãƒ¼ã‚¹ãƒšãƒ¼ã‚¸ã‹ã‚‰æœ€æ–°PDF URLã‚’æ¢ã™
    """
    from bs4 import BeautifulSoup  # requirements: beautifulsoup4

    page_url = "https://jta.or.jp/member/keiei/kit_release.html"
    html = requests.get(page_url, timeout=30).text
    soup = BeautifulSoup(html, "html.parser")

    pdf_links = []
    for a in soup.find_all("a"):
        href = a.get("href") or ""
        if "/pdf/kit_release/" in href and href.endswith(".pdf"):
            if href.startswith("http"):
                pdf_links.append(href)
            else:
                pdf_links.append("https://jta.or.jp" + href)

    if not pdf_links:
        raise ValueError("WebKIT PDFãƒªãƒ³ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆãƒšãƒ¼ã‚¸æ§‹é€ å¤‰æ›´ã®å¯èƒ½æ€§ï¼‰")

    def key(u: str) -> str:
        m = re.search(r"/(\d{6})\.pdf$", u)
        return m.group(1) if m else "000000"

    pdf_links = sorted(set(pdf_links), key=key, reverse=True)
    return pdf_links[0]

@st.cache_data(ttl=60 * 60)
def fetch_webkit_index_from_pdf(pdf_url: str) -> pd.Series:
    """
    PDFã‹ã‚‰æŒ‡æ•°ã£ã½ã„æ•°åˆ—ï¼ˆæœ€å¤§12å€‹ï¼‰ã‚’æ‹¾ã£ã¦ã€ç›´è¿‘12ãƒ¶æœˆã®æ™‚ç³»åˆ—ã«ã™ã‚‹
    â€»PDFã®å½¢å¼å¤‰æ›´ã«å¼±ã„ã®ã§ã€ãƒ€ãƒ¡ãªã‚‰ warning è¡¨ç¤ºã§æ­¢ã‚ã‚‹
    """
    import pdfplumber  # requirements: pdfplumber

    r = requests.get(pdf_url, timeout=60)
    r.raise_for_status()

    candidates = []

    with pdfplumber.open(io.BytesIO(r.content)) as pdf:
        for page in pdf.pages:
            # 1) è¡¨æŠ½å‡ºã‚’å„ªå…ˆ
            tables = page.extract_tables() or []
            for tbl in tables:
                if not tbl:
                    continue
                joined = " ".join([" ".join([c or "" for c in row]) for row in tbl])
                if "æœˆ" not in joined and "æŒ‡æ•°" not in joined and "æˆç´„" not in joined:
                    continue
                for row in tbl:
                    row_text = " ".join([c or "" for c in row])
                    nums = re.findall(r"\b\d{2,3}\b", row_text)
                    if len(nums) >= 8:
                        candidates.append(nums)

            # 2) ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºï¼ˆä¿é™ºï¼‰
            text = page.extract_text() or ""
            for ln in text.splitlines():
                nums = re.findall(r"\b\d{2,3}\b", ln)
                if len(nums) >= 8 and ("æŒ‡æ•°" in ln or "æˆç´„" in text or "é‹è³ƒ" in text):
                    candidates.append(nums)

    if not candidates:
        return pd.Series(dtype="float64")

    nums = max(candidates, key=len)
    nums = list(map(int, nums))[:12]

    m = re.search(r"/(\d{6})\.pdf$", pdf_url)
    if not m:
        return pd.Series(dtype="float64")

    yyyymm = m.group(1)
    base = pd.to_datetime(yyyymm + "01", format="%Y%m%d")
    idx = pd.date_range(end=base, periods=len(nums), freq="MS")
    return pd.Series(nums, index=idx).sort_index()

# ----------------------------
# Build base df (Copper/Aluminum)
# ----------------------------
copper = fetch_fred("PCOPPUSDM")
aluminum = fetch_fred("PALUMUSDM")
usdjpy = fetch_fred("EXJPUS")

df = pd.concat([copper, aluminum, usdjpy], axis=1, join="inner")
df.columns = ["copper_usd_ton", "aluminum_usd_ton", "usdjpy"]

df["copper_jpy_kg"] = df["copper_usd_ton"] * df["usdjpy"] / 1000
df["aluminum_jpy_kg"] = df["aluminum_usd_ton"] * df["usdjpy"] / 1000

# ----------------------------
# Latest / KPI
# ----------------------------
latest_date = df.index[-1]
latest_month_str = latest_date.strftime("%Y-%m")
latest_copper = float(df.loc[latest_date, "copper_jpy_kg"])
latest_aluminum = float(df.loc[latest_date, "aluminum_jpy_kg"])

delta_copper = None
delta_aluminum = None
if len(df) >= 2:
    prev_date = df.index[-2]
    delta_copper = latest_copper - float(df.loc[prev_date, "copper_jpy_kg"])
    delta_aluminum = latest_aluminum - float(df.loc[prev_date, "aluminum_jpy_kg"])

st.subheader("ğŸ“Œ æœ€æ–°æœˆã®åŸä¾¡ï¼ˆå††/kgï¼‰")
k1, k2, k3 = st.columns([1, 1, 1])
with k1:
    st.metric(
        label=f"éŠ…ï¼ˆ{latest_month_str}ï¼‰",
        value=f"{latest_copper:,.0f} å††/kg",
        delta=f"{delta_copper:+,.0f} å††/kg" if delta_copper is not None else None,
    )
with k2:
    st.metric(
        label=f"ã‚¢ãƒ«ãƒŸï¼ˆ{latest_month_str}ï¼‰",
        value=f"{latest_aluminum:,.0f} å††/kg",
        delta=f"{delta_aluminum:+,.0f} å††/kg" if delta_aluminum is not None else None,
    )
with k3:
    st.metric(
        label="æ›´æ–°æ—¥æ™‚",
        value=datetime.now().strftime("%Y-%m-%d %H:%M"),
    )

st.divider()

# ----------------------------
# Charts
# ----------------------------
def plot_with_latest_highlight(series: pd.Series, title: str, y_label: str):
    fig, ax = plt.subplots()
    ax.plot(series.index, series.values)
    ax.scatter(series.index[-1], series.values[-1], s=80, zorder=3)
    ax.annotate(
        f"{series.values[-1]:,.0f}",
        (series.index[-1], series.values[-1]),
        textcoords="offset points",
        xytext=(8, 8),
        ha="left",
    )
    ax.set_title(title)
    ax.set_xlabel("Month")
    ax.set_ylabel(y_label)
    ax.grid(True, alpha=0.3)
    st.pyplot(fig, clear_figure=True)

tab1, tab2, tab3, tab4, tab5 = st.tabs(
    ["ğŸŸ  éŠ…", "âšª ã‚¢ãƒ«ãƒŸ", "ğŸ“‰ ã¾ã¨ã‚", "ğŸ’´ è³ƒé‡‘ï¼ˆè£½é€ æ¥­ï¼‰", "ğŸšš ãƒˆãƒ©ãƒƒã‚¯é‹è³ƒæŒ‡æ•°"]
)

with tab1:
    st.subheader("éŠ… ä¾¡æ ¼æ¨ç§»ï¼ˆå††/kgï¼‰")
    plot_with_latest_highlight(df["copper_jpy_kg"], "Copper (JPY/kg)", "JPY/kg")
    st.caption("ãƒ‡ãƒ¼ã‚¿ï¼šFRED PCOPPUSDMï¼ˆUSD/tonï¼‰ã¨ EXJPUSï¼ˆUSDJPYï¼‰ã‹ã‚‰æ›ç®—")

with tab2:
    st.subheader("ã‚¢ãƒ«ãƒŸ ä¾¡æ ¼æ¨ç§»ï¼ˆå††/kgï¼‰")
    plot_with_latest_highlight(df["aluminum_jpy_kg"], "Aluminum (JPY/kg)", "JPY/kg")
    st.caption("ãƒ‡ãƒ¼ã‚¿ï¼šFRED PALUMUSDMï¼ˆUSD/tonï¼‰ã¨ EXJPUSï¼ˆUSDJPYï¼‰ã‹ã‚‰æ›ç®—")

with tab3:
    st.subheader("éŠ…ãƒ»ã‚¢ãƒ«ãƒŸï¼ˆå††/kgï¼‰åŒä¸€ã‚°ãƒ©ãƒ•")
    st.line_chart(df[["copper_jpy_kg", "aluminum_jpy_kg"]])
    st.caption("ã–ã£ãã‚Šæ¯”è¼ƒã—ãŸã„äººå‘ã‘ï¼ˆè©³ç´°ã¯å„ã‚¿ãƒ–ã¸ï¼‰")

with tab4:
    st.subheader("è£½é€ æ¥­ã®è³ƒé‡‘ï¼ˆe-Statï¼‰")
    if not ESTAT_APP_ID:
        st.info("ESTAT_APP_ID ãŒæœªè¨­å®šã®ãŸã‚ã€è³ƒé‡‘ãƒ‡ãƒ¼ã‚¿ã¯è¡¨ç¤ºã—ã¾ã›ã‚“ã€‚")
    else:
        # ã‚ãªãŸãŒè²¼ã£ã¦ãã‚ŒãŸURLã® stat_infid ã‚’ statsDataId ã¨ã—ã¦è©¦ã™
        stats_data_id = "000040277086"
        estat_json = fetch_estat_statsdata(stats_data_id)

        gsd = estat_json.get("GET_STATS_DATA", {})
        if "STATISTICAL_DATA" not in gsd:
            st.error("e-Stat APIãŒ STATISTICAL_DATA ã‚’è¿”ã—ã¦ã„ã¾ã›ã‚“ï¼ˆå–å¾—å¤±æ•—ï¼‰")
            st.write("RESULT:", gsd.get("RESULT"))
            st.write("ERROR_MSG:", gsd.get("ERROR_MSG"))
            st.stop()

        # ã¾ãšã¯ â€œå‹•ãå„ªå…ˆâ€ ã®æŠ½å‡ºï¼ˆè£½é€ æ¥­ã‚’å«ã‚€ç³»åˆ—ã‚’æ‹¾ã†ï¼‰
        wage_mfg = estat_pick_series(estat_json, industry_label_contains="è£½é€ æ¥­", item_label_contains=None)

        if wage_mfg.empty:
            st.warning("è£½é€ æ¥­ã®ç³»åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆçµ±è¨ˆè¡¨ID/æŠ½å‡ºæ¡ä»¶ã®èª¿æ•´ãŒå¿…è¦ã§ã™ï¼‰")
        else:
            st.line_chart(wage_mfg.rename("wage_mfg"))
            st.caption("â€»ã¾ãšã¯è£½é€ æ¥­ã‚’å«ã‚€ç³»åˆ—ã‚’æ¨å®šæŠ½å‡ºã—ã¦ã„ã¾ã™ã€‚æ¬¡ã«ã€ç¾é‡‘çµ¦ä¸ç·é¡ï¼ˆå††ï¼‰ã€ã«çµã‚‹èª¿æ•´ãŒå¯èƒ½ã§ã™ã€‚")

with tab5:
    st.subheader("å›½å†…ãƒˆãƒ©ãƒƒã‚¯é‹è³ƒæŒ‡æ•°ï¼ˆWebKIT æˆç´„é‹è³ƒæŒ‡æ•°ï¼‰")
    try:
        pdf_url = get_latest_webkit_pdf_url()
        st.caption(f"å–å¾—å…ƒPDF: {pdf_url}")

        webkit = fetch_webkit_index_from_pdf(pdf_url)
        if webkit.empty:
            st.warning("PDFã‹ã‚‰æŒ‡æ•°ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸï¼ˆPDFå½¢å¼å¤‰æ›´ã®å¯èƒ½æ€§ï¼‰")
        else:
            st.line_chart(webkit.rename("webkit_freight_index"))
    except Exception as e:
        st.error("å–å¾—å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
        st.write(str(e))

st.divider()

# ----------------------------
# Data table + download (Copper/Aluminum)
# ----------------------------
st.subheader("ğŸ“„ ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼šéŠ…ãƒ»ã‚¢ãƒ«ãƒŸï¼‰")
download_df = df[["copper_jpy_kg", "aluminum_jpy_kg"]].copy()
download_df = download_df.rename(
    columns={
        "copper_jpy_kg": "copper_jpy_per_kg",
        "aluminum_jpy_kg": "aluminum_jpy_per_kg",
    }
)
download_df.index.name = "date"

with st.expander("è¡¨ã‚’è¡¨ç¤º"):
    st.dataframe(download_df.tail(24), use_container_width=True)

csv_bytes = download_df.to_csv(encoding="utf-8-sig").encode("utf-8-sig")
st.download_button(
    label="ğŸ“¥ CSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
    data=csv_bytes,
    file_name="copper_aluminum_jpy_per_kg_monthly.csv",
    mime="text/csv",
)
